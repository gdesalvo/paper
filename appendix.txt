\appendix
\section{Definitions}\label{app:definitions}
This appendix contains tables of the definitions needed for UCB-N, UCB-D, and UCB-H. 
\begin{framed}
\textbf{Definitions for UCB-N.}
\begin{align*}
& T_j(t)= \sum_{s=1}^t 1_{I_s=j}\\
& Q_j(t)=\sum_{s=1}^t  \left(1_{r_{I_s}(x_s) \leq  0}1_{r_j(x_s) \leq 0} + 1_{r_{I_s}(x_s) > 0}\right) \\
& \widehat{\mu}_{j,t} = \frac{1}{Q_j(t)} \sum_{s=1}^t L( h_j, r_j,x_s, y_s) \big(1_{r_{I_s}(x_s) \leq 0} 1_{r(x_s) \leq 0} \\
& \hspace{20mm} + 1_{r_{I_s}(x_s) > 0}\big) \\
&\mathcal{S}_{j,t} = \sqrt{\frac{ \beta \log(t)}{2Q_j(t)}} 
\end{align*}
\end{framed}

\begin{framed}
\textbf{Definitions for UCB-D and UCB-H.}
\begin{align*}
& \mathcal{P}_{j}^+(t)=\frac{1}{t}\sum_{s=1}^t 1_{r_j(x_s)>0} \\
& \mathcal{P}_{j}^-(t)=\frac{1}{t} \sum_{s=1}^t 1_{r_j(x_s)\leq 0} \\
& O_j(t) = \sum_{s=1}^t 1_{r_j(x_s)>0}1_{r_{I_s}(x_s)>0} \\
& \widehat{\nu}_{j,t} = \frac{1}{O_j(t)} \sum_{s=1}^t 1_{y_s h_j(x_s) \leq 0} 1_{r_j(x_s)>0} 1_{r_{I_s}(x_s)>0}   \\
& \mathscr{S}_{t}= \sqrt{\frac{\beta \log t}{2t} }\\
& \widetilde{\mathscr{S}}_{j,t}= \sqrt{\frac{\beta \log t}{2O_j(t)} }\\
\end{align*}
\end{framed}


\section{Proofs of theorems in the adversarial setting}
This section contains the proof of the theorem of the adversarial setting of Section~\ref{sec:adversarial}. We first prove the guarantee of EXP3.Abs and then that of EXP3-IX.Abs.
\begin{reptheorem}{th:exp3rej}
For any $T\geq 1$ and any $(h^\star, r^\star) \in \mathcal{E}$, EXP3.Abs admits the guarantee: 
\begin{align*}
&  \E  \Big [ \sum_{t=1}^T \sum_{(h,r)\in V} p_t(h,r) L_t(h,r) \Big ] - \sum_{t=1}^T L_t(h^\star, r^\star) \leq \gamma T +  \\
& \frac{\log K}{\eta}+ \eta \sum_{t=1}^T \Bigg ( 8c^2 \log \frac{4K K_{abs,t}}{\gamma }  + 8\bar{c}^2 \log \frac{4K K_{acc,t}}{\gamma} \Bigg )
\end{align*}
\end{reptheorem}
\begin{proof}
By applying the standard second-order regret bound of
Hedge in \cite{CesaBianchiMansourStoltz2007} to distributions
$q_1,\ldots, q_t$ 
generated by EXP3.Abs and to the non-negative loss estimates $\widehat{l}_t(h,r)$, the following holds
\begin{align}
&\E \Bigg [ \sum_{t=1}^T \sum_{(h,r)\in V} q_t(h,r) \E_t\Big[\widehat{l}_t(h,r)\Big]- \sum_{t=1}^T \E_t\Big[\widehat{l}_t(h^*,r^*)\Big] \Bigg ] \nonumber \\
& \leq \frac{\log K}{\eta} + \eta \sum_{t=1}^T \E \left [  \sum_{i\in V} q_t(i) \E_t\left[\widehat{l}_t(h,r)^2\right] \right ] \nonumber     
\end{align}
for any fixed $(h^*,r^*) \in V$. Using the fact that $\E_t\Big[\widehat{l}_t(h,r)\Big]=L_t( h,r)$ and $ \E_t\Big[\widehat{l}_t(h,r)^2\Big]=\frac{L_t( h,r)^2}{P_t(h,r)}$,
\begin{align*}
& \E \Bigg [ \sum_{t=1}^T \sum_{(h,r)\in V} q_t(h,r) L_t( h,r)- \sum_{t=1}^T L_t( h^*,r^*)\Bigg ]\\
& \leq \frac{\log K}{\eta} + \eta \sum_{t=1}^T \E \Bigg [  \sum_{(h,r)\in V} \frac{q_t(h,r)}{P_t(h,r)} L_t( h,r)^2 \Bigg ]     
\end{align*}
Now for each $t$, we can split the graph in $V$ into two graphs $V_{abs,t}$ and $V_{acc,t}$ where if a node $i$ is abstaining then $i \in V_{abs,t}$ and if a node $j$ is accepting, then $j \in V_{acc,t}$. Thus, we can write the following
\begin{align*}
&\sum_{(h,r)\in V} \frac{q_t(h,r)}{P_t(h,r)} L_t( h,r)^2  \\
&= \sum_{(h,r)\in V_{abs,t}} \frac{q_t(h,r)}{P_t(h,r)} L_t( h,r)^2 \\& + \sum_{(h,r)\in V_{acc,t}} \frac{q_t(h,r)}{P_t(h,r)} L_t(h,r)^2 \\
& \leq  \sum_{(h,r)\in V_{abs,t}} \frac{q_t(h,r)}{P_t(h,r)}c^2+ \sum_{(h,r)\in V_{acc,t}} \frac{q_t(h,r)}{P_t(h,r)}\bar{c}^2
\end{align*}
The inequality holds since for the abstained points at time $t$, we know that the $L(h,r)=c$ while for the accepted points, we know that $L( h,r)\leq \bar{c}$. Since all nodes $(h,r)\in V_{abs,t}$ have self-loops and $p_t(h,r)\geq \frac{\gamma}{K}$ because we mixed with the uniform distribution, we can apply Lemma 5 in  \cite{AlonCesaBianchiDekelKoren2015} with $\epsilon=\frac{\gamma}{K}$. Thus, we have the following inequalities
\begin{align*}
\sum_{(h,r)\in V_{abs,t}} \frac{q_t(h,r)}{P_t(h,r)}c^2 &\leq 2 \sum_{(h,r)\in V_{abs,t}} \frac{p_t(h,r)}{P_t(h,r)}c^2 \\
&\leq 8c^2 \alpha_{abs,t} \log \frac{4K K_{abs,t}}{\gamma \alpha_{abs,t}} 
\end{align*}
where the first inequality is due to the fact that $p_t(h,r)\geq (1-\gamma)q_t(h,r)\geq \frac{1}{2}q_t(h,r)$ and where $\alpha_{abs,t}=\alpha(V_{abs,t})$ is the independence number of $V_{abs,t}$. Similarly, 
\begin{align*}
\sum_{(h,r)\in V_{acc,t}} \frac{q_t(h,r)}{P_t(h,r)}\bar{c}^2 &\leq 2 \sum_{(h,r)\in V_{acc,t}} \frac{p_t(h,r)}{P_t(h,r)}\bar{c}^2 \\
&\leq 8 \alpha_{acc,t} \bar{c}^2\log \frac{4K K_{acc,t}}{\gamma \alpha_{acc,t}}
\end{align*}
where $\alpha_{acc,t}=\alpha(V_{acc,t})$ is the independence number of $V_{acc,t}$.
Now, for the feedback graphs in this setting, the $\alpha_{abs,t}=\alpha_{acc,t}=1$ for all $t$, which is the best we can hope for. Thus,  
\begin{align*}
 &\sum_{(h,r)\in V_{abs,t}} \frac{q_t(h,r)}{P_t(h,r)}c^2+ \sum_{(h,r)\in V_{acc,t}} \frac{q_t(h,r)}{P_t(h,r)}\bar{c}^2 \\ & \leq  8c^2 \log \frac{4K K_{abs,t}}{\gamma }  + 8 \bar{c}^2 \log \frac{4K K_{acc,t}}{\gamma}
\end{align*}
Using then the fact that
\begin{align*}
&\sum_{(h,r)\in V} p_t(h,r)L_t(h,r)  \leq \sum_{(h,r)\in V} q_t(h,r) L_t(h,r) + \gamma
\encal{E}} \frac{p_t(e) \left[1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} + 1_{r(x_t)<0} c \right]}{P_t(e) + \kappa}\tilde{L}_t(e)\\
&\sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\tilde{L}_t(e) \\
&\quad + \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa}\tilde{L}_t(e) \\
& = \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}L_t(e) + \\
&\quad + \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa}L_t(e)  + \frac{K\log\left(\frac{K}{\delta}\right)}{\kappa} \\
&\leq \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c}^2 }{P_t(e) + \kappa} + \\
&\quad + \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r(x_t)<0} c^2 }{P_t(e) + \kappa} + \frac{K\log\left(\frac{K}{\delta}\right)}{\kappa}. \\
\end{align*}

Putting all the pieces together, we can now say that
\begin{align*}
&\sum_{t=1}^T L_t(e_t) \leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} \\
&\quad + \sum_{t=1}^T \sum_{e \in \mathcal{E}} p_t(e) L_t(e) \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right]  \\ 
&\quad + \sum_{t=1}^T \sum_{e \in \mathcal{E}}\frac{\kappa p_t(e) L_t(e)}{P_t(e) + \kappa} + \sum_{t=1}^T \tilde{L}_t(e^*) + \frac{\log K}{\eta}\\ 
&\quad + \frac{\eta}{2} \Bigg[\sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c}^2 }{P_t(e) + \kappa} + \\
&\quad + \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r(x_t)<0} c^2 }{P_t(e) + \kappa} + \frac{K\log\left(\frac{K}{\delta}\right)}{\kappa} \Bigg].
\end{align*}

Now consider the second term in the upper bound: $\sum_{t=1}^T \sum_{e \in \mathcal{E}} p_t(e) L_t(e) \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right]$. We can decompose this as follows:
\begin{align*}
& \sum_{e \in \mathcal{E}} p_t(e) L_t(e) \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right] \\
&= \sum_{e \in \mathcal{E}} p_t(e)  \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right] \\
&\quad + \sum_{e \in \mathcal{E}} p_t(e)  c 1_{r(x_t) < 0}  \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right]\\
&= X_t(e) + Y_t(e).
\end{align*}
Notice that each of $X_t(e)$ and $Y_t(e)$ is a martingale-difference with $|X_t| \leq K \bar{c}$ and $|Y_t| \leq K c$. 

Moreover, we can bound the conditional variance as:
\begin{align*}
&\E\Bigg[X_t(e)^2 | \mathcal{F}_t \Bigg] \\
&= \hspace*{-0.5mm} \E \hspace*{-0.5mm} \Bigg[\Bigg( \hspace*{-0.5mm} \sum_{e \in \mathcal{E}} p_t(e)  \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa} \Bigg)^2 \Bigg| \mathcal{F}_t \Bigg] \\
&\leq \E\Bigg[\Bigg( \sum_{e \in \mathcal{E}} p_t(e)  \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0} \left[ \frac{1_{e \in O_t}}{P_t(e) + \kappa}  \right] \Bigg)^2 \Bigg| \mathcal{F}_t \Bigg] \\
&=\hspace*{-0.5mm} \E \hspace*{-0.5mm}\Bigg[\hspace*{-0.5mm}\sum_{e, e' \in \mathcal{E}}  \hspace*{-1.5mm} \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e)p_t(e')1_{e \in O_t}1_{e' \in O_t}}{(P_t(e) \hspace*{-0.5mm}+\hspace*{-0.5mm} \kappa)(P_t(e') \hspace*{-0.5mm}+ \hspace*{-0.5mm}\kappa)}   \Bigg | \mathcal{F}_t \Bigg] \\
&\leq \hspace*{-0.5mm}\E \hspace*{-0.5mm}\Bigg[\sum_{e, e' \in \mathcal{E}}  \hspace*{-1.5mm} \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e)p_t(e')1_{e \in O_t}}{(P_t(e)\hspace*{-0.5mm} +\hspace*{-0.5mm} \kappa)(P_t(e')\hspace*{-0.5mm} + \hspace*{-0.5mm}\kappa)}   \Bigg | \mathcal{F}_t \Bigg] \\
&= \sum_{e, e' \in \mathcal{E}}   \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e)p_t(e')P_t(e)}{(P_t(e) + \kappa)(P_t(e') + \kappa)}   \\
&\leq \sum_{e, e' \in \mathcal{E}}  p_t(e) \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e')}{P_t(e') + \kappa} 
\end{align*}

By Freedman's inequality, it follows that with probability at least $1-\delta$, for any $\gamma \leq \frac{1}{K \bar{c}}$,
\begin{align*}
&\sum_{t=1}^T X_t(e) \leq \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} \\
&\quad + (e - 2) \gamma \sum_{t=1}^T \sum_{e' \in \mathcal{E}} \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e')}{P_t(e') + \kappa}.
\end{align*}
We can bound term $Y_t$ similarly to say that with
probability at least $1-\delta$, for any $\gamma' \leq \frac{1}{Kc}$,
\begin{align*}
&\sum_{t=1}^T Y_t(e) \leq \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'} \\
&\quad + (e - 2) \gamma' \sum_{t=1}^T \sum_{e' \in \mathcal{E}} c 1_{r(x_t) <0}  \frac{p_t(e')}{P_t(e') + \kappa}.
\end{align*}

It now follows that with probability at least $1 - 4 \delta$, for any $\gamma \leq \frac{1}{K \bar{c}}$ and  $\gamma' \leq \frac{1}{Kc}$,
\begin{align*}
&\sum_{t=1}^T L_t(e_t) \leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} \\
&\quad + \sum_{t=1}^T \sum_{e \in \mathcal{E}}\frac{\kappa p_t(e) L_t(e)}{P_t(e) + \kappa} + \sum_{t=1}^T \tilde{L}_t(e^*) + \frac{\log K}{\eta}\\ 
&\quad + \frac{\eta}{2} \Bigg[\sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c}^2 }{P_t(e) + \kappa} \\
&\quad + \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r(x_t)<0} c^2 }{P_t(e) + \kappa} + \frac{K\log\left(\frac{K}{\delta}\right)}{\kappa} \Bigg] \\
&\quad + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'} \\
&\quad + (e - 2) \gamma \sum_{t=1}^T \sum_{e' \in \mathcal{E}} \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e')}{P_t(e') + \kappa} \\
&\quad + (e - 2) \gamma' \sum_{t=1}^T \sum_{e' \in \mathcal{E}} c 1_{r(x_t) <0}  \frac{p_t(e')}{P_t(e') + \kappa} \\
&= \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} + \sum_{t=1}^T \tilde{L}_t(e^*) + \frac{\log K}{\eta}\\ 
& + \left(\frac{\eta}{2} \bar{c} + \kappa + (e-2) \gamma  \right) \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa} \\
&  + \left(\frac{\eta}{2} c + \kappa + (e-2) \gamma' \right) \sum_{t=1}^T \sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa} \\
&  + \frac{\eta}{2}\frac{K\log\left(\frac{K}{\delta}\right)}{\kappa} + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'}.
\end{align*}

By Lemma 1 in \citep{KocakNeuValkoMunos2014},
\begin{align*}
&\sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa} \leq \sum_{e\in \mathcal{E}_{\text{acc},t}} \frac{p_t(e) \bar{c} }{P_t(e) + \kappa} \\
&\leq 2 \bar{c} \left[\log\left( 1 + \left\lceil\frac{ K_{\text{acc},t}^2}{\kappa}\right\rceil +  K_{\text{acc},t}\right) + 1 \right],
\end{align*}
and 
\begin{align*}
&\sum_{e\in \mathcal{E}} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa} \leq \sum_{e\in \mathcal{E}_{\text{abs},t}} \frac{p_t(e) c }{P_t(e) + \kappa} \\
&\leq 2 c \left[\log\left( 1 + \left\lceil\frac{ K_{\text{abs},t}^2}{\kappa}\right\rceil +  K_{\text{abs},t}\right) + 1\right].
\end{align*}

Thus, we have shown that
\begin{align*}
&\sum_{t=1}^T L_t(e_t) \leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}}\\
& \quad + \sum_{t=1}^T \tilde{L}_t(e^*) + \frac{\log K}{\eta}\\ 
&\quad + \left(\frac{\eta}{2} \bar{c} + \kappa + (e-2) \gamma  \right) \\
&\quad \quad \sum_{t=1}^T 2 \bar{c} \left[\log\left( 1 + \left\lceil\frac{ K_{\text{acc},t}^2}{\kappa}\right\rceil +  K_{\text{acc},t}\right) + 1 \right] \\
&\quad + \left(\frac{\eta}{2} c + \kappa + (e-2) \gamma' \right) \\
&\quad \quad \sum_{t=1}^T 2 c \left[\log\left( 1 + \left\lceil\frac{ K_{\text{abs},t}^2}{\kappa}\right\rceil +  K_{\text{abs},t}\right) + 1\right] \\
&\quad + \frac{\eta}{2}\frac{K\log\left(\frac{K}{\delta}\right)}{\kappa} + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'} \\
&\leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} + \sum_{t=1}^T L_t(e^*) + \frac{\log K}{\eta}\\ 
&\quad + \left(\frac{\eta}{2} \bar{c} + \kappa + (e-2) \gamma  \right) \\
&\quad \quad \sum_{t=1}^T 2 \bar{c} \left[\log\left( 1 + \left\lceil\frac{ K_{\text{acc},t}^2}{\kappa}\right\rceil +  K_{\text{acc},t}\right) + 1 \right] \\
&\quad + \left(\frac{\eta}{2} c + \kappa + (e-2) \gamma' \right) \\
&\quad \quad \sum_{t=1}^T 2 c \left[\log\left( 1 + \left\lceil\frac{ K_{\text{abs},t}^2}{\kappa}\right\rceil +  K_{\text{abs},t}\right) + 1\right] \\
&\quad + \frac{3\eta }{4}\frac{K\log\left(\frac{K}{\delta}\right)}{\kappa} + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'}.
\end{align*}


\end{proof}


\section{Proofs of theorems in the stochastic setting}
In this section,  we prove the theorems  in the stochastic setting of Section~\ref{sec:stochastic} in the following order: UCB-N, UCB-D, and UCB-H. 

\begin{reptheorem}{th:ucbn}
The regret of the UCB-N after $T$ steps is upper bounded by
\begin{align*}
&\E \Bigg[ \sum_{t=1}^T L_t( h_{I_t}, r_{I_t}) - \sum_{t=1}^T  L_t( h_*, r_*)\Bigg]\\
&\leq \sum_{e_j \in \mathcal{E}} \left(  \frac{ 2\beta \log(T)T_j(\mathcal{T})}{(\mu_{h_j,r_j}-\mu_{h_*,r_*})Q_j(\mathcal{T})}  + \frac{\beta}{\beta -2} \right)
\end{align*}
where $\mathcal{T}=\argmax_t \frac{T_j(t)}{Q_j(t)}$.
\end{reptheorem}
\begin{proof}
The regret can be written as follows
\begin{align*}
&\E \Bigg[ \sum_{t=1}^T L_t( h_{I_t}, r_{I_t}) - \sum_{t=1}^T  L_t( h_*, r_*)\Bigg] \\
&=  \E \Bigg[ \sum_{t=1}^T \sum_{j=1}^K  \left (L_t( h_j, r_j) -   L_t( h_*, r_*)\right ) 1_{I_t=j}\Bigg] \\
&=  \sum_{t=1}^T \sum_{j=1}^K  \left ( \mu_{h_j, r_j} -  \mu_{h_*, r_*}\right ) \E  [ 1_{I_t=j} ]
\end{align*}
We then split the sum of the expectation according to $T_j(t-1)$, the number of times expert $j$ was chosen,
\begin{align*}
& \sum_{t=1}^T \E  [ 1_{I_t=j} ]\\
& =\sum_{t=1}^T \E  [ 1_{I_t=j} 1_{T_j(t-1)\leq s} ] + \sum_{t=1}^T \E  [ 1_{I_t=j}1_{T_j(t-1)> s}  ]  \\
 & \leq s + \sum_{t=s+1}^T \E  [ 1_{I_t=j}1_{T_j(t-1)> s}  ]  \\
\end{align*}
Now, if expert $j$ is chosen, that is $I_t=j$, then it must have had the lowest confidence bound  $\widehat{\mu}_{j,t-1}- \mathcal{S}_{j,t-1} \leq \widehat{\mu}_{i,t-1}- \mathcal{S}_{i,t-1}  $ for all $i\in \mathcal{E}$.  Thus, 
\begin{align} \label{eq:larges}
&\E  [ 1_{I_t=j}1_{T_j(t-1)> s}  ] \nonumber  \\
& \leq  \Pr [\forall i\in \mathcal{E},\widehat{\mu}_{j,t-1}\hspace{-1mm}- \hspace{-1mm}\mathcal{S}_{j,t-1} \hspace{-1mm}\leq \hspace{-1mm}\widehat{\mu}_{i,t-1}\hspace{-1mm}-\hspace{-1mm} \mathcal{S}_{i,t-1},T_j(t\hspace{-1mm}-\hspace{-1mm}1)\hspace{-1mm}>\hspace{-1mm}s ] \nonumber \\
&\leq  \Pr [\widehat{\mu}_{j,t-1}- \mathcal{S}_{j,t-1} \leq \widehat{\mu}_{*,t-1}- \mathcal{S}_{*,t-1} ,T_j(t-1)> s ]
\end{align}

where the last inequality comes from the fact that the probability of an intersection of events is less than the probability of one event. 
Now, we can re-write the inequality as
\begin{align*}
0 &\leq \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1}- \widehat{\mu}_{j,t-1}+\mathcal{S}_{j,t-1}\\ &=-\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} + 2\mathcal{S}_{j,t-1} - (\mu_{h_j,r_j}  \\ & - \mu_{h_*,r_*}) + \mu_{h_j,r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1}, 
\end{align*}
that is it can be written as the sum of the following three term
\begin{align*}
  [1]\hspace{4mm} & -\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} \\
  [2] \hspace{4mm}&  2\mathcal{S}_{j,t-1} - (\mu_{h_j,r_j}- \mu_{h_*,r_*}) \\
  [3] \hspace{4mm} & \mu_{h_j,r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1}  \\
\end{align*}
Note that at least one of the three terms is non-negative.  If 
$$s= \ceil*{ \frac{2\beta \log T}{ (\mu_{h_j,r_j}- \mu_{h_*,r_*})^2 }\max_t \frac{T_j(t-1)}{Q_j(t-1)} },$$ then
$T_j(t-1) \geq s \geq  \frac{2\beta \log t}{ (\mu_{h_j,r_j}- \mu_{h_*,r_*}) ^2 }\frac{T_j(t-1)}{Q_j(t-1)}$. This leads to the following set of inequalities:

\begin{align*}
&Q_j(t-1) \geq  \frac{2\beta \log t}{ (\mu_{h_j,r_j}- \mu_{h_*,r_*})^2 } \\ &\implies (\mu_{h_j,r_j}- \mu_{h_*,r_*})^2 \geq  \frac{2\beta \log t}{ Q_j(t-1) } \\
&\implies (\mu_{h_j,r_j}- \mu_{h_*,r_*}) \geq  \sqrt{\frac{2\beta \log t}{ Q_j(t-1) }}  
\end{align*}

The last inequality can be written as $-2\mathcal{S}_{j,t} +(\mu_{h_j,r_j}- \mu_{h_*,r_*})\geq 0$. This implies that at least one of the [1] and [3] terms are non-negative. Thus, we can rewrite the left-hand side of Inequality~\ref{eq:larges} as
\begin{align*}
& \Pr \big[\widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1}\leq\widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t},T_j(t-1)>s   \big] \\
&\leq \Pr \big[ \mu_{h_j,r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1} \geq 0 \big ] \\ &  + \Pr \big[   -\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} \geq 0   \big]\\
\end{align*}

Now by Hoeffing's inequality and the union bound,
\begin{align*}
&\Pr \left( -\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} \geq 0\right)\\ 
& \leq \Pr \left ( \exists s\in [1,t]: -\mu_{h_*,r_*}+ \widehat{\mu}_{*,s}-\sqrt{\frac{2\beta \log t}{s}} \geq 0    \right )\\
&\leq \sum_{s=1}^t \frac{1}{t^\beta} = \frac{1}{t^{\beta -1}}
\end{align*}

A similar argument holds for the [3] term, that is $\Pr \big( \mu_{h_j,r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1} \geq 0\big) \leq \frac{1}{t^{\beta -1}} $ and thus
 \begin{align*}
& \sum_{t=1}^T   \E \big [ 1_{I_t=j} \big ]  \\ 
&\leq     \frac{2\beta \log T}{ (\mu_{h_j,r_j}- \mu_{h_*,r_*})^2 } \max_t \frac{T_j(t-1)}{Q_j(t-1)}  + 2  \sum_{t=s+1}^T \frac{1}{t^{\beta -1}}  \\ 
& \leq    \frac{2\beta \log T}{ (\mu_{h_j,r_j}- \mu_{h_*,r_*})^2 } \max_t  \frac{T_j(t-1)}{Q_j(t-1)}   + \frac{\beta}{\beta-2}\\
&  \leq \frac{2\beta \log T}{ (\mu_{h_j,r_j}- \mu_{h_*,r_*})^2 }   \frac{T_j(\mathcal{T})}{Q_j(\mathcal{T})}   + \frac{\beta}{\beta-2}
 \end{align*}
 
Using this upper bound of the expectation in the equation $ \sum_{t=1}^T \sum_{j=1}^K  \left ( \mu_{h_j, r_j} -  \mu_{h_*, r_*}\right ) \E  [ 1_{I_t=j} ]$, we can then conclude the statement of the theorem.

\end{proof}

\begin{theorem}
\label{th:ucbd}
The regret of \textsc{UCB-D} after $T$ rounds is upper bounded by:
\begin{align*}
&\sum_{t=1}^T \E[L_t(h_t, r_t)] - \E[L_t(h^*, r^*)] \\
&\leq \sum_{i=1}^K \Bigg(\frac{2 \beta \log(T) 4(2 + \bar{c} + c)^2}{\Delta_i} \max_{t} \frac{N_i(t-1)}{O_i(t-1)} \\
&\quad + \frac{3\beta}{\beta - 2} \Bigg).
\end{align*}
\end{theorem}
While the bound for \textsc{UCB-D} is on the same order as the one for \textsc{UCB-N}, we expect the actual performance of this algortihm to be better in scenarios where it is often the case that the chosen arm abstains on an example that many of the other experts would not have abstained. 
In these cases, the estimates in \textsc{UCB-N} would not be updated.
However, \textsc{UCB-D} estimates the probability of abstention separately from the classification error. Thus, the algorithm would still learn and refines the loss estimates. We verify this observation in our experiments. 

\begin{proof}
We begin by decomposing the regret in the standard way:
\begin{align*}
&\sum_{t=1}^T \E[L_t(h_t, r_t)] - \E[L_t(h^*, r^*)] \\
&= \sum_{t=1}^T \sum_{i=1}^K \E[(L_t(h_i, r_i) - L_t(h^*, r^*))1_{(h_t,r_t) = (h,r)}] \\
&= \sum_{t=1}^T \sum_{i=1}^K (\mu_{(h_i,r_i)} - \mu_{(h^*,r^*)}) \E[1_{(h_t,r_t) =(h_i,r_i)}] \\
&= \sum_{i=1}^K \Delta_i \left(s_i + \E\left[\sum_{t=s_i + 1}^T 1_{(h_t,r_t) =(h_i,r_i), N_i(t-1) > s_i}\right] \right).
\end{align*}
As in \textsc{UCB-N}, if expert $(h_i,r_i)$ is chosen at round $t$, then it must have had the lowest confidence bound. Specifically, for all $j \in [K]$, it must be the case that:
\begin{align*}
&\left(\widehat{\nu}_{i,t-1} - \tilde{\mathscr{S}}_{i, t-1} \right) \left(\mathcal{P}_i^+(t-1) - \mathscr{S}_{t-1} \right) \\
&\quad + c \left(\mathcal{P}_i^-(t-1) - \mathscr{S}_{t-1} \right) \\
&\leq \left(\widehat{\nu}_{j,t-1} - \tilde{\mathscr{S}}_{j, t-1} \right) \left(\mathcal{P}_j^+(t-1) - \mathscr{S}_{t-1} \right) \\
&\quad + c \left(\mathcal{P}_j^-(t-1) - \mathscr{S}_{t-1} \right), \\
\end{align*}
and this must be true in particular for $j = i^*$. 

For simplicity of notation, let $\mathcal{L}_{i,t} = \left(\widehat{\nu}_{i,t} - \tilde{\mathscr{S}}_{i, t} \right) \left(\mathcal{P}_i^+(t) - \mathscr{S}_{t} \right) + c \left(\mathcal{P}_i^-(t) - \mathscr{S}_{t} \right)$.
Then, we can rewrite the above condition in the following way:
\begin{align*}
0 
&\leq \mathcal{L}_{i^*,t-1} - \mathcal{L}_{i, t-1} \\ 
&= \mathcal{L}_{i^*,t-1} - \E[L(e_{i^*})] \\
&\quad + \E[L(e_i)] - \mathcal{L}_{i, t-1} \\ 
&\quad + \E[L(e_{i^*}) - \E[L(e_i)] \\
&= \mathcal{L}_{i^*,t-1} - \E[L(e_{i^*})] \\
&\quad + \E[L(e_i)] - \mathcal{L}_{i, t-1} - 2 (2 + \bar{c} + c) \tilde{\mathscr{S}}_{j,t-1} \\ 
&\quad + \E[L(e_{i^*}) - \E[L(e_i)] + 2 (2 + \bar{c} + c) \tilde{\mathscr{S}}_{j,t-1}. 
\end{align*}

Thus, at least one of the three terms must be positive. 

Now let $s_i = \frac{2 \beta \log(T) 4(2 + \bar{c} + c)^2}{\Delta_i^2} \max_{t} \frac{N_i(t-1)}{O_i(t-1)}$. Notice that if $N_i(t-1) > s_i$, then it must be the case that
\begin{align*}
	N_i(t-1) > s_i \geq  \frac{2 \beta \log(T) 4 (2 + \bar{c} + c)^2}{\Delta_i^2} \frac{N_i(t-1)}{O_i(t-1)}, 
    \end{align*}
    which implies that 
    \begin{align*}
    \Delta_i \geq \sqrt{\frac{2 \beta \log(T) 4 (2 + \bar{c} + c)^2}{O_i(t-1)}},
    \end{align*}
so that we must have
$$\E[L(e_{i^*}) - \E[L(e_i)] + 2 (2 + \bar{c} + c) \tilde{\mathscr{S}}_{j,t-1} \leq 0.$$

Thus, it must be the case that either 
$$\mathcal{L}_{i^*,t-1} - \E[L(e_{i^*})] \geq 0,$$
or 
$$\E[L(e_i)] - \mathcal{L}_{i, t-1} - 2 (2 + \bar{c} + c) \tilde{\mathscr{S}}_{j,t-1} \geq 0.$$

By using the fact that
\begin{align*}
&\mathbb{P}[\mathcal{L}_{i^*,t-1} - \E[L(e_{i^*})] \geq 0] \\
&= \mathbb{P}\Bigg[\left(\widehat{\nu}_{i^*,t-1} - \tilde{\mathscr{S}}_{i^*, t-1} \right) \left(\mathcal{P}_{i^*}^+(t-1) - \mathscr{S}_{t-1} \right) \\
&\quad + c \left(\mathcal{P}_{i^*}^-(t-1) - \mathscr{S}_{t-1} \right) \geq \E[L(e_{i^*})] \Bigg] \\
&\leq \mathbb{P}\left[\left(\widehat{\nu}_{i^*,t-1} - \tilde{\mathscr{S}}_{i^*, t-1} \right) \geq \bar{c} \E[1_{y h_{i^*}(x) < 0}] \right] \\
&\quad + \mathbb{P} \left[\left(\mathcal{P}_{i^*}^+(t-1) - \mathscr{S}_{t-1} \right) \geq \E[1_{r_{i^*}(x)>0}] \right] \\
&\quad + c \mathbb{P} \left[ \left(\mathcal{P}_{i^*}^-(t-1) - \mathscr{S}_{t-1} \right) \geq \E[L(e_{i^*})] \geq \E[1_{r_{i^*}(x)<0}] \right] \\
&\leq \sum_{s=1}^{t-1} \frac{3}{t^{\beta}}.
\end{align*}
The last line follows from the union bound over the number of observations 
$O_{i^*}(t-1)$ along with the definitions of $\tilde{\mathscr{S}}_{i^*, t-1}$
and $\mathscr{S}_{ t-1}$.

For the other term, we can compute that:
\begin{align*}
&\mathbb{P}\Bigg[\E[L(h_i, r_i)] - \mathcal{L}_{i, t-1} - 2 (2 + \bar{c} + c) \tilde{\mathscr{S}}_{j,t-1} \geq 0\Bigg] \\
&= \mathbb{P}\Bigg[\E[L(e_i)] - \mathcal{L}_{i, t-1} \geq 2 (2 + \bar{c} + c) \tilde{\mathscr{S}}_{j,t-1} \Bigg] \\
&\leq \mathbb{P}\Bigg[\E[L(e_i)] - \mathcal{L}_{i, t-1} \geq 2 \Big[ \mathscr{S}_{i,t-1} \left( \mathcal{P}_i^+(t-1) + \mathscr{S}_{t-1} \right) \\
&\quad + \mathscr{S}_{t-1} \left( \widehat{\nu}_{i,t} + c \right) \Big] \Bigg],
\end{align*}
and by expanding the definition of $\mathcal{L}_{i,t-1}$ we can rewrite this expression as: 
\begin{align*}
& \mathbb{P}\Bigg[\E[L(h_i, r_i)] \geq \left(\widehat{\nu}_{i,t-1} - \tilde{\mathscr{S}}_{i, t-1} \right) \left(\mathcal{P}_{i}^+(t-1) - \mathscr{S}_{t-1} \right) \\
&\quad + c \left(\mathcal{P}_{i}^-(t-1) - \mathscr{S}_{t-1} \right) + 2 \Big[ \mathscr{S}_{i,t-1} \left( \mathcal{P}_i^+(t-1) + \mathscr{S}_{t-1} \right) \\
&\quad + \mathscr{S}_{t-1} \left( \widehat{\nu}_{i,t} + c \right) \Big] \Bigg] \\
&= \mathbb{P}\Bigg[E[L(h_i,r_i)] \geq  \left(\widehat{\nu}_{i,t-1} + \tilde{\mathscr{S}}_{i, t-1} \right) \left(\mathcal{P}_{i}^+(t-1) + \mathscr{S}_{t-1} \right) \\
&\quad + c \left(\mathcal{P}_{i}^-(t-1) + \mathscr{S}_{t-1} \right)\Bigg] \\
&\leq \mathbb{P} \Bigg[ \bar{c} \E[1_{yh_i(x)<0}] \geq \widehat{\nu}_{i,t-1} + \tilde{\mathscr{S}}_{i, t-1} \Bigg] \\
&\quad + \mathbb{P} \Bigg[ \E[1_{r_i(x)>0} \geq \mathcal{P}_{i}^+(t-1) + \mathscr{S}_{t-1} ] \Bigg] \\
&\quad + \mathbb{P} \Bigg[ \E[1_{r_i(x)<0} \geq \mathcal{P}_{i}^-(t-1) + \mathscr{S}_{t-1} \Bigg] \\
&\leq \sum_{s=1}^{t-1} \frac{3}{t^\beta}.
\end{align*}
The last line again follows from a union bound over the number of observations $O_i(t-1)$ along with the definition of $\tilde{\mathscr{S}}_{i,t-1}$ and $\mathscr{S}_{t-1}$. 

Thus, we have shown that 
\begin{align*}
\E\left[\sum_{t=s_i + 1}^T 1_{(h_t,r_t) =(h_i,r_i), N_i(t-1) > s_i}\right]
&\leq \sum_{t=s_i + 1}^T \frac{6}{t^{\beta-1}}\\
&\leq \frac{3\beta}{\beta - 2}
\end{align*}

Combining this with the definition of $s_i$ shows that:
\begin{align*}
&\sum_{t=1}^T \E[L_t(h_t, r_t)] - \E[L_t(h^*, r^*)] \\
&\leq \sum_{i=1}^K \Bigg(\frac{2 \beta \log(T) 4(2 + \bar{c} + c)^2}{\Delta_i} \max_{t} \frac{N_i(t-1)}{O_i(t-1)} \\
&\quad + \frac{3\beta}{\beta - 2} \Bigg).
\end{align*}

\end{proof}

\begin{reptheorem}{th:ucbh}
For any $T>0$, the sleeping regret of UCB-H is upper bounded by
\begin{align*}
& \sum_{t=1}^T \E [ L_t( h_{I_t}, r_{I_t} )]  - \min_{\sigma} \sum_{t=1}^T \E [ L_t( h_{\sigma(W_t)}, r_{\sigma(W_t)} )]\\
& \leq O \left(  \beta \log T  \sum_{j=2}^{K}\frac{1}{\nu_{j}-\nu_{j-1}} \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})}  \right )  
\end{align*}
where $\mathcal{T}=\argmax_t \frac{T_j(t)}{O_j(t)}$
\end{reptheorem}
\begin{proof}
We initially analyze the regret by focusing on each expert $j$. For $[i]=\{1,2,\ldots,i\}$, let $M_{i,j}$ for $i<j$ denote the number of time expert $j$ was chosen, where some expert in $[i]$ could have been chosen. Then, 
\begin{align*}
\E[M_{i,j}]= \sum_{t=1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }]
\end{align*}
where $W_t$ is the set of awake experts. Suppose for now that expert $e_j$ is not the all-abstain function $e_\diamond$. We can split the sum over the rounds according to $T_j(t-1)$ the number of times that $j$ was chosen:
\begin{align*}
&\sum_{t=1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }]\\ &= \sum_{t=1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{T_j(t-1) \leq s }] \\ &+ \sum_{t=1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{T_j(t-1) > s }] \\
&  \leq s + \sum_{t=s+1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{T_j(t-1) > s }] 
\end{align*}

Now, if expert $j$ was chosen, that is $I_t=j$, then it must have had the lowest confidence bound $\widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1}  $ for all $e_k\in W_t-e_\diamond$ and $ \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq c$.  Let $e_{k^*}=\argmin_{e_k \in W_t \wedge [i] }C(k) $ where $C(k)= \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1} $  if  $e_k\in W_t \wedge [i] - \{e_\diamond \}$ and $C(k)=c$ if $e_\diamond \in  W_t \wedge [i]$. 
\begin{align}\label{eq:largeawakes}
& E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{T_j(t-1) > s }] \nonumber
\\  & \leq \Pr[ \forall e_k\in W_t-e_\diamond, \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1},\nonumber \\
&\hspace{10mm} \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1}\leq c ,W_t \wedge [i]\neq \emptyset,T_j(t-1) > s ]\nonumber \\
& \leq \Pr[ \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq C(k^*) ,\nonumber\\
&\hspace{10mm} W_t \wedge [i]\neq \emptyset,T_j(t-1) > s ]  
\end{align}
where the last inequality comes from the fact that the probability of an intersection of events is less than the probability of one event. If $C(k^*)= \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}$, then we can write the inequality 
\begin{align*}
0& \leq \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}-  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1} \\
& =\widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}-  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1}- \nu_{k^*}+ \nu_{k^*} \\
& - \nu_{j}+ \nu_{j} - \nu_{i}+ \nu_{i}   + \mathscr{S}_{j,t-1} - \mathscr{S}_{j,t-1}\\
& =- \nu_{k^*}+ \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1} + \nu_{j}-  \widehat{\nu}_{j,t-1} - \mathscr{S}_{j,t-1}\\
&+ (\nu_{k^*} - \nu_{i}) - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1} \\
\end{align*}
as the sum of the following three terms
\begin{align*}
  [1]\hspace{4mm} & -\nu_{k^*}+ \widehat{\nu}_{k^*,t-1}-\mathscr{S}_{k^*,t-1} \\
  [2] \hspace{4mm}&  - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1}  +(\nu_{k^*} - \nu_{i})\\
  [3] \hspace{4mm} & \nu_{j}- \widehat{\nu}_{j,t-1}-\mathscr{S}_{j,t-1}  \\
\end{align*}
Note that at least one of the three terms in non-negative.  
If instead $C(k^*)=c$, then we can write the inequality 
\begin{align*}
0& \leq c-  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1} \\
& =c -  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1} - \nu_{j}+ \nu_{j} \\
&  - \nu_{i}+ \nu_{i}   + \mathscr{S}_{j,t-1} - \mathscr{S}_{j,t-1}\\
& =  \nu_{j}-  \widehat{\nu}_{j,t-1} - \mathscr{S}_{j,t-1}\\
& + (c - \nu_{i}) - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1} \\
\end{align*}
as the sum of the following two terms 
\begin{align*}
  [1'] \hspace{4mm} & \nu_{j}- \widehat{\nu}_{j,t-1}-\mathscr{S}_{j,t-1}  \\
  [2'] \hspace{4mm}&  - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1}  +(c - \nu_{i})\\
\end{align*}

By  choosing $$s=\ceil*{\frac{2\beta \log T}{(\nu_j-\nu_i)^2} \max_t \frac{T_j(t-1)}{O_j(t-1)} },$$  then $T_j(t-1) \geq s \geq \frac{2\beta \log t}{(\nu_j-\nu_i)^2} \frac{T_j(t-1)}{O_j(t-1)}  $. This leads to the following set of inequalities:

\begin{align*}
&O_j(t-1) \geq  \frac{2\beta \log t}{ (\nu_{j}- \nu_{i})^2 } \\ &\implies (\nu_{j}- \nu_{i})^2 \geq  \frac{2\beta \log t}{ O_j(t-1) } \\
&\implies (\nu_{j}- \nu_{i}) \geq  \sqrt{\frac{2\beta \log t}{ O_j(t-1) }} \\
&\implies (\nu_{j}- \nu_{i}) - 2\mathscr{S}_{j,t-1}  \geq 0 
\end{align*}
Notice that by definition of the set $[i]$, $\nu_{k^*} \leq \nu_i$ as well as $c\leq \nu_i$, which implies $-(\nu_{k^*}-\nu_i)\geq 0$ and $-(c-\nu_i)\geq 0$. Thus $$(\nu_{j}- \nu_{i}) - 2\mathscr{S}_{j,t-1}-(\nu_{k^*}-\nu_i)  \geq 0 $$ and  $$(\nu_{j}- \nu_{i}) - 2\mathscr{S}_{j,t-1}-(c-\nu_i)  \geq 0 $$ which is equivalent to term [2] and [2'] being non-positive. Thus, at least one of the  [1] and [3] terms are non-negative and also [1'] must be non-negative.  This implies that we can re-write the right-hand side of Inequality~\ref{eq:largeawakes}  as
\begin{align*}
 &\Pr[ \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq C(k^*), \\
&\hspace{10mm} W_t \wedge [i]\neq \emptyset,T_j(t-1) > s ]  \\
& \leq  \Pr[-\nu_{k^*}+ \widehat{\nu}_{k^*,t-1}-\mathscr{S}_{k^*,t-1}  \geq 0 ] \\ 
& \hspace{2mm} +\Pr[\nu_{j}- \widehat{\nu}_{j,t-1}-\mathscr{S}_{j,t-1}\geq 0]
\end{align*}
Now by Hoeffding's inequality and the union bound, 
\begin{align*}
&\Pr \big( -\nu_{k^*}+ \widehat{\nu}_{k^*,t-1}-\mathscr{S}_{k^*,t-1} \geq 0\big)\\ 
& \leq \Pr  \left ( \exists s\in [1,t]: -\nu_{k^*}+ \widehat{\nu}_{k^*,s}-\sqrt{\frac{2\beta \log t}{s}} \geq 0    \right )\\
&\leq \sum_{s=1}^t \frac{1}{t^\beta} = \frac{1}{t^{\beta -1}}
\end{align*}
A similar argument holds for  [3] and [1'] term and thus,
\begin{align*}
&\E[M_{i,j}] \\
& \leq  \frac{2\beta \log T}{(\nu_j-\nu_i)^2} \max_t \frac{T_j(t-1)}{O_j(t-1)} + 2 \sum_{t=s+1}^T \frac{1}{t^{\beta-1}}\\ 
& \leq  \frac{2\beta \log T}{(\nu_j-\nu_i)^2}  \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})} + \frac{\beta}{\beta-2}\\ 
\end{align*}

We now assume that expert $j=j_\diamond$ is the all-abstain expert. Thus if expert $j_\diamond$ is chosen, then $c \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1}  $ for all $e_k\in W_t-e_\diamond$. Let $e_{k^*}=\argmin_{e_k \in W_t \wedge [i]}  \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1} $ so that
\begin{align*}
 & \E[1_{I_t=j_\diamond}1_{W_t \wedge [i]\neq \emptyset}]\\ 
 & \leq \Pr[\forall e_k \in W_t-e_\diamond, c \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1},W_t \wedge [i]\neq \emptyset  ]\\
 & \leq  \Pr[  c \leq \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}, W_t \wedge [i]\neq \emptyset  ]
\end{align*}
We can re-write the inequality as
\begin{align*}
& 0 \leq \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1} -c \\
& = - \nu_{k^*} + \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1} -c +\nu_{k^*} \\
\end{align*}
Since by definition of the set $[i]$ and the fact that $j_\diamond>i$, $c -\nu_{k^*}\geq 0$, then  $- \nu_{k^*} + \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}\geq 0$. Similarly as before, by Hoeffding's and the union bound, $\Pr [- \mu_{k^*} + \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}\geq 0] \leq \frac{1}{t^{\beta-1}}$. Thus, when expert $j$ is the all-abstain function, for $i<j$,
\begin{align*}
\E[M_{i,j_\diamond} ] & = \sum_{t=1}^T \E[1_{I_t=j_\diamond}1_{W_t \wedge [i]\neq \emptyset}]\\ & \leq \sum_{t=1}^T \frac{1}{t^{\beta-1}} \leq \frac{\beta}{2(\beta-2)}
\end{align*}

Now that we have bounded the expectation of $M_{i,j}$, we turn to the sleeping regret, which can be written as
\begin{align*}
&\E \left [\sum_{j=2}^{K} \sum_{i=1}^{j-1} ( M_{i,j}-  M_{i-1,j} ) (\nu_j-\nu_i) \right ] \\
&=\E \left [ \sum_{j=2}^{K} \sum_{i=1}^{j-1}  M_{i,j}  (\nu_{i+1}-\nu_i) \right ]\\
\end{align*}
by simply regrouping terms and using Lemma~\ref{lemma:sleepingregretreform}. 
Then by using the bound on the expectation of $M_{i,j}$, 
\begin{align*}
& \sum_{j=2}^{K} \sum_{i=1}^{j-1}  \E[M_{i,j}] (\nu_{i+1}-\nu_i) \\
&\leq \sum_{j=2}^{K} 2\beta \log T \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})}  \sum_{i=1}^{j-1} \frac{1}{(\nu_j-\nu_i)^2} (\nu_{i+1}-\nu_i) \\ &- 2\beta \log T \frac{P_{j_\diamond}(\mathcal{T})}{O_{j_\diamond}(\mathcal{T})}  \sum_{i=1}^{{j_\diamond}-1} \frac{1}{(\nu_{j_\diamond}-\nu_i)^2} (\nu_{i+1}-\nu_i)  +O(1)
\end{align*}
where we added and subtracted the factor $$2\beta \log T \frac{P_{j_\diamond}(\mathcal{T})}{O_{j_\diamond}(\mathcal{T})}  \sum_{i=1}^{{j_\diamond}-1} \frac{1}{(\nu_{j_\diamond}-\nu_i)^2} (\nu_{i+1}-\nu_i)$$ of the all-abstain expert and used the fact that $E[M_{i,j_\diamond}]=O(1)$.  Note that $O_{j_\diamond}$ is the number of times the all-abstain expert was observed and $P_{j_\diamond}$ is the number of times it was chosen.
We now focus on the first term $ \sum_{j=1}^{K-1} 2\beta \log T \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})}  \sum_{i=1}^{j-1} \frac{1}{(\nu_j-\nu_i)^2} (\nu_{i+1}-\nu_i)$, which  we can write it as
\begin{align*}
 2\beta \log T \sum_{j=1}^{K-1} \sum_{i=1}^{j-1} \frac{(\nu_{i+1}-\nu_i ) }{(\nu_j-\nu_i)^2}  \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})} 
\end{align*}
We then apply Lemma ~\ref{lemma3}  to conclude the statement of the theorem. 

TODO: find better way to keep track to $j_\diamond$
\end{proof}

\begin{lemma}\label{lemma3}
For $\nu_1 \leq \nu_2 \leq \ldots \leq \nu _K$, then
\begin{align*}
& \sum_{j=2}^{K} \sum_{i=1}^{j-1}  \frac{(\nu_{i+1}-\nu_i ) }{(\nu_j-\nu_i)^2}  \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})}  \\
& \leq 2 \sum_{j=2}^{K}\frac{1}{\nu_{j}-\nu_{j-1}} \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})} \\
\end{align*}
\end{lemma}
\begin{proof}
Consider the following simple change of variable
\begin{align*}
&\sum_{j=2}^{K } \sum_{i=1}^{j-1} \frac{(\nu_{i+1}-\nu_i )}{(\nu_j-\nu_i)^2}  \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})} \\ 
&=\sum_{j=2}^{K } \sum_{i=1}^{j-1} \frac{(\nu -\nu + \nu_{i+1}-\nu_i )}{(\nu -\nu + \nu_j-\nu_i)^2} \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})} \\
&= \sum_{j=2}^{K } \sum_{i=1}^{j-1} \frac{(\nu -\nu_i -(\nu - \nu_{i+1}) ) }{(\nu-\nu_i - (\nu - \nu_j))^2}\frac{T_j(\mathcal{T})}{O_j(\mathcal{T})} \\
\end{align*}
where $\nu > \max_i \nu_i $. Letting $\tilde{\nu}_i= \nu -\nu_i$,  then $\tilde{\nu}_1 \geq \tilde{\nu}_2 \geq \ldots \geq  \tilde{\nu}_K $ and 
\begin{align*}
& \sum_{i=1}^{j-1} \frac{(\nu -\nu_i -(\nu T- \nu_{i+1}) ) }{(\nu-\nu_i - (\nu - \nu_j))^2}  \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})}\\
& =\sum_{i=1}^{j-1} \frac{(\tilde{\nu}_i -\tilde{\nu}_{i+1} ) }{(\tilde{\nu}_i - \tilde{\nu}_j)^2} \frac{T_j(\mathcal{T})}{O_j(\mathcal{T})} \\
\end{align*}
so that we can directly apply Lemma~\ref{lemma:newlemma3}
\end{proof}

\begin{lemma} \label{lemma:newlemma3}
For $\mu_1 \geq \mu_2 \geq \ldots \geq \mu _K$, let $F_j>0$ for all $j\in[1,K]$ and define $\Delta_{i,j}=\mu_i-\mu_j$, then
\begin{align*}
& \sum_{j=2}^{K} \sum_{i=1}^{j-1} \frac{F_j}{\Delta_{i,j}^2}  \Delta_{i,i+1}\leq 2\sum_{j=2}^K F_j \Delta_{j-1,j}^{-1}  \\
\end{align*}
\end{lemma}
\begin{proof}
The following proof is adapted from Lemma 3 of \citet{KleinbergNiculescuSharma2008} to include the factor $F_j$. By changing the order of summation,
\begin{align*}
 &\sum_{j=2}^{K} \sum_{i=1}^{j-1} \frac{F_j}{\Delta_{i,j}^2}  \Delta_{i,i+1} = \sum_{i=1}^{K-1} \Delta_{i,i+1} \sum_{j:j>i}\frac{F_j}{\Delta_{i,j}^2}
\end{align*}
We then analyze the second sum by fixing an expert $i\in[n]$,  
\begin{align*}
&\sum_{j:j>i}\frac{F_j}{\Delta_{i,j}^2}\\ &= \sum_{j=2}^K 1_{j>i}\frac{F_j}{\Delta_{i,j}^2} \\
& =\int_{x=0}^\infty \#\{j:j>i,\Delta_{i,j}^{-2}F_j \geq x \}dx \\
& = \int_{x=0}^\infty \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq x^{-1/2} \}dx \\
& = -2 \int_{y=\infty}^0 \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq y \}y^{-3}dy \\
& = 2 \int_{y=0}^\infty \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq y \}y^{-3}dy \\
\end{align*}
where we changed variables of integration $x^{-1/2}=y$.

Thus combining the above, 
\begin{align*}
 & \sum_{i=1}^{K-1} \Delta_{i,i+1} \sum_{j:j>i}\frac{F_j}{\Delta_{i,j}^2} \\
 & = 2\sum_{i=1}^{K-1}  \Delta_{i,i+1}  \int_{y=0}^\infty \hspace*{-1mm} \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq y \}y^{-3}dy  \\
 & =   2  \hspace*{-1mm}  \int_{y=0}^\infty \hspace*{-2mm}  y^{-3}  \left( \sum_{i=1}^{K-1}  \Delta_{i,i+1}  \#\{j\hspace*{-1mm}:\hspace*{-1mm}j\hspace*{-1mm}>\hspace*{-1mm}i,\Delta_{i,j}F_j^{-1/2} \leq y \}  \hspace*{-1mm}  \right)  dy \\
\end{align*}
where we changed the order of integration and summation. We then expand $\#\{\}$ into a sum of indicator functions
\begin{align*}
 &  2  \hspace*{-1mm}  \int_{y=0}^\infty \hspace*{-2mm}  y^{-3}  \left( \sum_{i=1}^{K-1}  \Delta_{i,i+1}  \#\{j\hspace*{-1mm}:\hspace*{-1mm}j\hspace*{-1mm}>\hspace*{-1mm}i,\Delta_{i,j}F_j^{-1/2} \leq y \}  \hspace*{-1mm}  \right)  dy \\
& = 2  \int_{y=0}^\infty y^{-3}  \left( \sum_{i=1}^{K-1}  \Delta_{i,i+1}  \sum_{j=i+1}^K 1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y}  \right)  dy \\
& = 2  \int_{y=0}^\infty y^{-3}  \left(\sum_{j=2}^K   \sum_{i=1}^{j-1}  \Delta_{i,i+1}  1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y}  \right)  dy \\
\end{align*}
where the last equation is by changing the order of summation.
For an expert $j$ and $y\geq 0$, we define 
\begin{align*}
i_{y}(j):=\argmin \{i: i\leq j, \Delta_{i,j}F_j^{-1/2}\leq y \}
\end{align*}
In other words, $i_{y}(j)$ is the the minimum numbered expert $i\leq j$ such that $\Delta_{i,j}F_j^{-1/2}$ is no more than $y$. Thus,  
\begin{align*}
&\sum_{j=2}^K   \sum_{i=1}^{j-1}  \Delta_{i,i+1}  1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y} \\
&=\sum_{j=2}^K   \sum_{i=i_{y}(j)}^{j-1}  \Delta_{i,i+1} \\
& =\sum_{j=2}^K  ( \mu_{i_{y}(j)} - \mu_j)
\end{align*}
By using this fact and changing the order of summation and integration
\begin{align*}
 &2  \int_{y=0}^\infty y^{-3}  \left(\sum_{j=2}^K   \sum_{i=1}^{j-1}  \Delta_{i,i+1}  1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y}  \right)  dy \\
 & = 2\sum_{j=2}^K \int_{y=0}^\infty y^{-3} ( \mu_{i_{y}(j)} - \mu_j)\\
 & =  2\sum_{j=2}^K \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-3} ( \mu_{i_{y}(j)} - \mu_j) \\
\end{align*}
where the last equality holds because for values y less than $\Delta_{j-1,j}F_j^{-1/2}$, $i_y(j)=j$ and the integrand is equal to zero. Notice also that $( \mu_{i_{y}(j)} - \mu_j) F_j^{-1/2} \leq y $, hence
\begin{align*}
& 2\sum_{j=2}^K \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-3} ( \mu_{i_{y}(j)} - \mu_j)\\
& = 2\sum_{j=2}^K \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-3} y F_j^{1/2} dy\\
& = 2\sum_{j=2}^K  F_j^{1/2}  \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-2}  dy\\
& =2\sum_{j=2}^K  F_j  \Delta_{j-1,j}^{-1}\\
\end{align*}
which is the bound the lemma. 
\end{proof}


\begin{lemma}\label{lemma:sleepingregretreform}
The sleeping regret can be written in terms of the conditional expectations in the following way
\begin{align*}
& \sum_{t=1}^T \E[ L_t ( h_{I_t}, r_{I_t} )] - \min_{\sigma} \sum_{t=1}^T \E [ L_t ( h_{\sigma(W_t)}, r_{\sigma(W_t)} )]\\
&=\E \left [\sum_{j=2}^{K} \sum_{i=1}^{j-1} ( M_{i,j}-  M_{i-1,j} ) (\nu_j-\nu_i) \right ] \\
\end{align*}
where $M_{i,j}$ for $i<j$ is the number of time expert $j$ was chosen, where some expert in $[i]$ could have been chosen.
\end{lemma}
\begin{proof}
Let $I^*_t$ be the index of the best ranked expert that is available at time $t$. 
\begin{align*}
&\sum_{t=1}^T \E[ L_t ( h_{I_t}, r_{I_t} )] - \min_{\sigma} \sum_{t=1}^T \E [ L_t ( h_{\sigma(W_t)}, r_{\sigma(W_t)} )] \\
&= \E \left[ \sum_{t=1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1} 1_{I_t=j}1_{I^*_t=i}[L_t ( h_{j}, r_{j} ) -  L_t ( h_i, r_i )]  \right] \\
&= \sum_{t=1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1} \E \left[ 1_{I_t=j}1_{I^*_t=i}[L_t ( h_{j}, r_{j} ) -  L_t ( h_i, r_i )]  \right] \\
\end{align*}
We now analyze $\E \left[ 1_{I_t=j}1_{I^*_t=i}[L_t ( h_{j}, r_{j} ) -  L_t ( h_i, r_i )]  \right] $ and assume for now that $j$ and $i$ are not the all-reject expert. Then,
\begin{align*}
&\E \left[ 1_{I_t=j}1_{I^*_t=i}[L_t ( h_{j}, r_{j} ) -  L_t ( h_i, r_i )]  \right] \\
& = \E \left[ 1_{I_t=j}1_{I^*_t=i}L_t ( h_{j}, r_{j} )\right ] - \E \left[ 1_{I_t=j}1_{I^*_t=i} L_t ( h_i, r_i )  \right]\\
& = \E \left[ 1_{I_t=j}1_{I^*_t=i}L_t ( h_{j}, r_{j} )|r_j>0\right ]\Pr(r_j>0)\\ 
& \quad - \E \left[ 1_{I_t=j}1_{I^*_t=i} L_t ( h_i, r_i ) |r_i>0 \right]\Pr[r_i>0]\\
\end{align*}
where we used the fact that expert $i$ and expert $j$ are not the all-reject function and hence they are only chosen if they accept, which implies that $\E \left[ 1_{I_t=j}1_{I^*_t=i} L_t ( h_i, r_i ) |r_i\leq 0 \right]=0 $ and $\E \left[ 1_{I_t=j}1_{I^*_t=i} L_t ( h_j, r_j ) |r_j\leq 0 \right]=0 $. Since the decision of $I_t$ is independent of the loss once we know whether an arm accepts or rejects,
\begin{align*}
& \E \left[ 1_{I_t=j}1_{I^*_t=i}L_t ( h_{j}, r_{j} )|r_j>0\right ]\Pr(r_j>0) \\
&\quad - \E \left[ 1_{I_t=j}1_{I^*_t=i} L_t ( h_i, r_i ) |r_i>0 \right]\Pr[r_i>0]\\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i} |r_j>0\right ] \E \left[L_t ( h_{j}, r_{j} )|r_j>0\right ]\Pr(r_j>0) \\
&\quad- \E \left[ 1_{I_t=j}1_{I^*_t=i}|r_i>0 \right] \E \left[ L_t ( h_i, r_i ) |r_i>0 \right]\Pr[r_i>0] \\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ] \E \left[L_t ( h_{j}, r_{j} )|r_j>0\right ] \\ &\quad - \E \left[ 1_{I_t=j}1_{I^*_t=i}  \right] \E \left[ L_t ( h_i, r_i ) |r_i>0 \right]  \\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ]  ( \E \left[L_t ( h_{j}, r_{j} )|r_j>0\right ] \\ 
&\quad - \E \left[ L_t ( h_i, r_i ) |r_i>0 \right]  )  \\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ] \left ( \nu_j -\nu_i\right )  \\
\end{align*}
Thus,  
\begin{align*}
& \sum_{t=1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1}  \E \left[ 1_{I_t=j}1_{I^*_t=i}[L_t ( h_{j}, r_{j} ) -  L_t ( h_i, r_i )]  \right]\\
 & = \sum_{t=1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1} \E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ] \left ( \nu_j -\nu_i\right ) 
\end{align*}
It is then easy to conclude the statement in the theorem. 

Suppose instead that expert $j$ is the all-reject, then 
\begin{align*}
&\E \left[ 1_{I_t=j}1_{I^*_t=i}[L_t ( h_{j}, r_{j} ) -  L_t ( h_i, r_i )]  \right]\\ 
& =\E \left[ 1_{I_t=j}1_{I^*_t=i}L_t ( h_{j}, r_{j} )|r_j\leq 0\right] \Pr(r_j\leq 0) \\ &\quad - \E \left[ 1_{I_t=j}1_{I^*_t=i} L_t ( h_i, r_i )|r_i>0\right ]\Pr(r_i>0)  
\end{align*}
and the rest of the proof proceed in a similar way as above. 
\end{proof}


\section{Follow-the-Leader in the Stochastic Setting}
In this section, we show the regret bound of Follow-the-Leader (FTL) algorithm in the stochastic setting. At each round, FTL chooses the expert with the smallest empirical loss and updates the empirical loss of all the experts. See Algorithm~\ref{alg:ftl} for the pseudocode. 

\begin{algorithm}
  \caption{Follow-the-Leader }         
\label{alg:ftl}      
\begin{algorithmic}  
 \FOR{$t \geq 1$}
 \STATE $(h_{I_t},r_{I_t}) \leftarrow \argmin_{(h,r)\in \mathcal{E}} \Big \{ \widehat{L}_{h,r,t} \Big \}$
 \FOR {$(h',r')\in \mathcal{E}$}
  \STATE $\widehat{L}_{h',r',t+1} \leftarrow \frac{ L_t(h',r')}{t+1} + (1-\frac{1}{t+1}) \widehat{L}_{h',r',t}$ 
 \ENDFOR
 \ENDFOR
\end{algorithmic}
\end{algorithm}
\begin{theorem}
The regret of the FTL after $T$ steps is upper bounded by
\begin{align*}
& \bE \left[ \sum_{t=1}^T  L_t( h_t,r_t) -  \min_{(h,r)\in\mathcal{E}}  L_t(h,r)  \right] \\ & \leq  \frac{4 \max_{(h_j,r_j)\in \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*})  }{\min_{(h_j,r_j)\in \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*}) ^2} 
\end{align*} 
\end{theorem}
\begin{proof}
The regret of the FTL algorithm can be written as
\begin{align*}
&\bE \Bigg [ \sum_{t=1}^T  (L_t( h_{I_t},r_{I_t}) - L_t( h^*,r^*) )1_{I_t\neq *} \Bigg ]\\
\leq & \sum_{t=1}^T \max_{(h_j,r_j)\in \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*}) \bE  [ 1_{I_t\neq *}] \\
\leq &\max_{(h_j,r_j)\in \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*})  \sum_{t=1}^T  \bE  [ 1_{I_t\neq *}] 
\end{align*}
Using Lemma 1 of  \citet{CaronKvetonLelargeBhagat2012}, the following holds
\begin{align*}
 \sum_{t=1}^T  \bE  [ 1_{I_t\neq *}] \leq &    \sum_{t=1}^T  2e^{-t\min_{(h_j,r_j)\in  \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*})^2 /2 } \\
 = &     \frac{2(1-e^{-T\min_{e_j\in  \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*})^2 /2 })}{e^{\min_{(h_j,r_j)\in  \mathcal{E}}  (\mu_{h_j,r_j}-\mu_{h_*,r_*})^2 /2 }-1}   \\
 \leq &  \frac{2}{e^{\min_{ (h_j,r_j)\in  \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*})^2 /2 }-1} \\
  \leq &  \frac{2}{\min_{(h_j,r_j)\in  \mathcal{E}} (\mu_{h_j,r_j}-\mu_{h_*,r_*})^2 /2 } 
\end{align*}
\end{proof}
\end{document} 
