\appendix
\section{Definitions}\label{app:definitions}
This appendix contains tables for the definitions needed for UCB-N, UCB-D, and UCB-H. 
\begin{framed}
\textbf{Definitions for UCB-N.}
\begin{align*}
& P_j(t)= \sum_{s=1}^t 1_{I_s=j}\\
& Q_j(t)=\sum_{s=1}^t  \left(1_{r_{I_s}(x_s) \leq  0}1_{r_j(x_s) \leq 0} + 1_{r_{I_s}(x_s) > 0}\right) \\
& \widehat{\mu}_{j,t} = \frac{1}{Q_j(t)} \sum_{s=1}^t L( h_j, r_j,x_s, y_s) \big(1_{r_{I_s}(x_s) \leq 0} 1_{r(x_s) \leq 0} \\
& \hspace{20mm} + 1_{r_{I_s}(x_s) > 0}\big) \\
&\mathcal{S}_{j,t} = \sqrt{\frac{ \beta \log(t)}{2Q_j(t)}} 
\end{align*}
\end{framed}

\begin{framed}
\textbf{Definitions for UCB-D and UCB-H.}
\begin{align*}
& \mathcal{P}_{j}^+(t)=\frac{1}{t}\sum_{s=1}^t 1_{r_j(x_s)>0} \\
& \mathcal{P}_{j}^-(t)=\frac{1}{t} \sum_{s=1}^t 1_{r_j(x_s)\leq 0} \\
& O_j(t) = \sum_{s=1}^t 1_{r_j(x_s)>0}1_{r_{I_s}(x_s)>0} \\
& \widehat{\nu}_{j,t} = \frac{1}{O_j(t)} \sum_{s=1}^t 1_{y_s h_j(x_s) \leq 0} 1_{r_j(x_s)>0} 1_{r_{I_s}(x_s)>0}   \\
& \mathscr{S}_{t}= \sqrt{\frac{\beta \log t}{2t} }\\
& \widetilde{\mathscr{S}}_{j,t}= \sqrt{\frac{\beta \log t}{2O_j(t)} }\\
\end{align*}
\end{framed}


\section{Proofs of theorems in the adversarial setting}

This section contains the proof of the theorem of the adversarial
setting of Section~\ref{sec:adversarial}. We first prove the guarantee
of EXP3.Abs and then that of EXP3-IX.Abs.
\begin{reptheorem}{th:exp3rej}
For any $T\geq 1$ and any $(h^\star, r^\star) \in \cE$, EXP3.Abs admits the guarantee: 
\begin{align*}
&  \E  \Big [ \sum_{t = 1}^T \sum_{(h, r)\in V} p_t(h, r) L(h, r) \Big ] - \sum_{t = 1}^T L(h^\star, r^\star) \leq \gamma T +  \\
& \frac{\log K}{\eta}+ \eta \sum_{t = 1}^T \Bigg ( 8c^2 \log \frac{4K K_{abs,t}}{\gamma }  + 8\bar{c}^2 \log \frac{4K K_{acc,t}}{\gamma} \Bigg )
\end{align*}
\end{reptheorem}
\begin{proof}
By applying the standard second-order regret bound of Hedge in
\cite{CesaBianchiMansourStoltz2007} to distributions $q_1,\ldots, q_t$
generated by EXP3.Abs and to the non-negative loss estimates
$\widehat{l}_t(h, r)$, the following holds
\begin{align}
&\E \Bigg [ \sum_{t = 1}^T \sum_{(h, r)\in V} q_t(h, r)
    \E_t\Big[\widehat{l}_t(h, r)\Big]- \sum_{t = 1}^T
    \E_t\Big[\widehat{l}_t(h^*,r^*)\Big] \Bigg ] \nonumber \\ & \leq
  \frac{\log K}{\eta} + \eta \sum_{t = 1}^T \E \left [ \sum_{i\in V}
    q_t(i) \E_t\left[\widehat{l}_t(h, r)^2\right] \right ] \nonumber
\end{align}
for any fixed $(h^*,r^*) \in V$. Using the fact that
$\E_t\Big[\widehat{l}_t(h, r)\Big]=L( h, r)$ and $
\E_t\Big[\widehat{l}_t(h, r)^2\Big]=\frac{L( h, r)^2}{P_t(h, r)}$,
\begin{align*}
& \E \Bigg [ \sum_{t = 1}^T \sum_{(h, r)\in V} q_t(h, r) L( h, r) - \sum_{t = 1}^T L( h^*,r^*)\Bigg ]\\
& \leq \frac{\log K}{\eta} + \eta \sum_{t = 1}^T \E \Bigg [  \sum_{(h, r)\in V} \frac{q_t(h, r)}{P_t(h, r)} L( h, r)^2 \Bigg ]     
\end{align*}
Now for each $t$, we can split the graph in $V$ into two graphs
$V_{abs,t}$ and $V_{acc,t}$ where if a node $i$ is abstaining then $i
\in V_{abs,t}$ and if a node $j$ is accepting, then $j \in
V_{acc,t}$. Thus, we can write the following
\begin{align*}
&\sum_{(h, r)\in V} \frac{q_t(h, r)}{P_t(h, r)} L( h, r)^2  \\
  &= \sum_{(h, r)\in V_{abs,t}} \frac{q_t(h, r)}{P_t(h, r)} L( h, r)^2 \\
  & + \sum_{(h, r)\in V_{acc,t}} \frac{q_t(h, r)}{P_t(h, r)} L(h, r)^2 \\
& \leq  \sum_{(h, r)\in V_{abs,t}} \frac{q_t(h, r)}{P_t(h, r)}c^2+ \sum_{(h, r)\in V_{acc,t}} \frac{q_t(h, r)}{P_t(h, r)}\bar{c}^2
\end{align*}
The inequality holds since for the abstained points at time $t$, we
know that the $L(h, r)=c$ while for the accepted points, we know that
$L( h, r)\leq \bar{c}$. Since all nodes $(h, r)\in V_{abs,t}$ have
self-loops and $p_t(h, r)\geq \frac{\gamma}{K}$ because we mixed with
the uniform distribution, we can apply Lemma 5 in
\cite{AlonCesaBianchiDekelKoren2015} with
$\epsilon=\frac{\gamma}{K}$. Thus, we have the following inequalities
\begin{align*}
\sum_{(h, r)\in V_{abs,t}} \frac{q_t(h, r)}{P_t(h, r)}c^2 &\leq 2 \sum_{(h, r)\in V_{abs,t}} \frac{p_t(h, r)}{P_t(h, r)}c^2 \\
&\leq 8c^2 \alpha_{abs,t} \log \frac{4K K_{abs,t}}{\gamma \alpha_{abs,t}} 
\end{align*}
where the first inequality is due to the fact that $p_t(h, r)\geq (1-\gamma)q_t(h, r)\geq \frac{1}{2}q_t(h, r)$ and where $\alpha_{abs,t}=\alpha(V_{abs,t})$ is the independence number of $V_{abs,t}$. Similarly, 
\begin{align*}
\sum_{(h, r)\in V_{acc,t}} \frac{q_t(h, r)}{P_t(h, r)}\bar{c}^2 &\leq 2 \sum_{(h, r)\in V_{acc,t}} \frac{p_t(h, r)}{P_t(h, r)}\bar{c}^2 \\
&\leq 8 \alpha_{acc,t} \bar{c}^2\log \frac{4K K_{acc,t}}{\gamma \alpha_{acc,t}}
\end{align*}
where $\alpha_{acc,t}=\alpha(V_{acc,t})$ is the independence number of $V_{acc,t}$.
Now, for the feedback graphs in this setting, the $\alpha_{abs,t}=\alpha_{acc,t}=1$ for all $t$, which is the best we can hope for. Thus,  
\begin{align*}
 &\sum_{(h, r)\in V_{abs,t}} \frac{q_t(h, r)}{P_t(h, r)}c^2+ \sum_{(h, r)\in V_{acc,t}} \frac{q_t(h, r)}{P_t(h, r)}\bar{c}^2 \\ & \leq  8c^2 \log \frac{4K K_{abs,t}}{\gamma }  + 8 \bar{c}^2 \log \frac{4K K_{acc,t}}{\gamma}
\end{align*}
Using then the fact that
\begin{align*}
&\sum_{(h, r)\in V} p_t(h, r)L(h, r)  \leq \sum_{(h, r)\in V} q_t(h, r) L(h, r) + \gamma
\end{align*}
the regret bound in this case can be written as
\begin{align*}
&\E \Bigg [ \sum_{t = 1}^T \sum_{(h, r)\in V} p_t(h, r) L(h, r) \Bigg ] - \sum_{t = 1}^T L(h^*,r^*)  \\
&\leq \gamma T + \frac{\log K}{\eta}\\ 
& + \eta \sum_{t = 1}^T \Bigg [ 8c^2 \log \frac{4K K_{abs,t}}{\gamma }  + 8 \bar{c}^2 \log \frac{4K K_{acc,t}}{\gamma} \Bigg ]
\end{align*}
\end{proof}


\begin{reptheorem}{th:exp3ixabs}
With probability at least $1-4\delta$, for any $\gamma \leq \frac{1}{|\cE| \bar{c}}$ and  $\gamma' \leq \frac{1}{|\cE|c}$, EXP3-IX.Abs admits the following guarantee:
\begin{align*}
&\sum_{t = 1}^T L_t(e_t) - L_t(e^*)\leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} + \frac{\log K}{\eta}\\ 
&\quad + \left(\frac{\eta}{2} \bar{c} + \kappa + (e-2) \gamma  \right)  \sum_{t = 1}^T 2 \bar{c} f(K_{\text{acc},t}) \\
&\quad + \left(\frac{\eta}{2} c + \kappa + (e-2) \gamma' \right)  \sum_{t = 1}^T 2 c f(K_{\text{abs},t}) \\
&\quad + \frac{3\eta }{4}\frac{K\log\left(\frac{K}{\delta}\right)}{\kappa} + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'}.
\end{align*}
where $f(x)= \log\left( 1 + \left\lceil\frac{x^2}{\kappa}\right\rceil + x\right) + 1  $.
\end{reptheorem}

\begin{proof}
We want to relate the losses that we incur, $L_t(e_t)$ to the 
surrogate losses $\tilde{L}_t(e_t)$. We do this in two steps. First, note that by applying Hoeffding's inequality,  
\begin{align*}
\sum_{t = 1}^T L_t(e_t)
\leq \sum_{t = 1}^T \sum_{e \in \cE} p_t(e) L_t(e) + \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}}. 
\end{align*}
Next, we can relate the expected loss to the expected surrogate loss: 
\begin{align*}
&\sum_{t = 1}^T \sum_{e \in \cE} p_t(e) L_t(e) \\
&= \sum_{t = 1}^T \sum_{e \in \cE} p_t(e) \tilde{L}_t(e) - p_t(e) \tilde{L}_t(e) + p_t(e) L_t(e) \\
&= \sum_{t = 1}^T \sum_{e \in \cE} p_t(e) \tilde{L}_t(e) \\
&\quad - \sum_{t = 1}^T \sum_{e \in \cE} p_t(e) L_t(e) \left[ \frac{1_{e \in O_t}}{P_t(e) + \kappa} - \frac{P_t(e) + \kappa}{P_t(e) + \kappa} \right]  \\ 
&=\sum_{t = 1}^T \sum_{e \in \cE} p_t(e) \tilde{L}_t(e) \\
&\quad + \sum_{t = 1}^T \sum_{e \in \cE} p_t(e) L_t(e) \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right]  \\ 
&\quad + \sum_{t = 1}^T \sum_{e \in \cE}\frac{\kappa p_t(e) L_t(e)}{P_t(e) + \kappa}.
\end{align*}

Now, since the algorithm plays a weighted majority algorithm on $\tilde{L}_t$, we can
bound the cumulative expected surrogate loss by:
\begin{align*}
&\sum_{t = 1}^T \sum_{e \in \cE} p_t(e) \tilde{L}_t(e) \\
&\leq \sum_{t = 1}^T \tilde{L}_t(e^*) + \frac{\log|\cE|}{\eta} + \frac{\eta}{2}\sum_{t = 1}^T \sum_{e \in \cE} p_t(e) \tilde{L}_t(e)^2.
\end{align*}

By definition of $\tilde{L}_t$, we can upper bound the last term as follows:
\begin{align*}
&\sum_{t = 1}^T \sum_{e \in \cE} p_t(e) \tilde{L}_t(e)^2 \\
&= \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) \left[1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} + 1_{r(x_t)<0} c \right]}{P_t(e) + \kappa}\tilde{L}_t(e)\\
&= \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\tilde{L}_t(e) \\
&\quad + \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa}\tilde{L}_t(e).
\end{align*}

We can bound each component in this last term in the following way:
\begin{align*}
&\frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\tilde{L}_t(e) \\
&= \frac{\frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}1_{e \in O_t}L_t(e)}{P_t(e) + \kappa}\\
&\leq \frac{\frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}1_{e \in O_t}L_t(e)}{P_t(e) + \kappa\frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}L_t(e)}\\
&= \frac{\frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}1_{e \in O_t}\frac{L_t(e)}{P_t(e)}}{1 + \kappa\frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\frac{L_t(e)}{P_t(e)}}\\
&\leq \frac{1}{2\kappa} \log\left( 1 + 2 \kappa\frac{L_t(e) 1_{e \in O_t}}{P_t(e)} \right).
\end{align*}
The last line follows from the fact that $\frac{x}{1+(x/2)} \leq \log(1+x)$ for $x \geq 0$.

Thus, it follows that
\begin{align*}
&\exp\left(2\kappa \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\tilde{L}_t(e)\right) \\
&\leq  1 + 2 \kappa\frac{L_t(e) 1_{e \in O_t}}{P_t(e)} 
\end{align*}
Let $(\mathcal{F}_t)_{t = 1}^\infty$ be a filtration such that $\mathcal{F}_t$ is the history
of the game up to knowledge of $x_t$. Then we can compute that
\begin{align*}
&\E\left[\exp\left(2\kappa \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\tilde{L}_t(e)\right) | \mathcal{F}_t \right] \\
&\leq \E\left[  1 + 2 \kappa\frac{L_t(e) 1_{e \in O_t}}{P_t(e)} | \mathcal{F}_t\right] \\
&\leq 1 + 2 \kappa L_t(e) \\
&\leq \exp\left( 2\kappa L_t(e) \right).
\end{align*}
The last line follows from the fact that $1+x \leq e^x$ for all $x \in \Rset$.

Thus, we can use this exponential bound to compute that
\begin{align*}
&\mathbb{P}\Bigg[\sum_{t = 1}^T 2 \kappa \frac{p_t(e)}{P_t(e) + \kappa}1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} \\
&\quad \left(\tilde{L}_t(e) - L_t(e) \right) > \frac{\log\left(\frac{|\cE|}{\delta}\right)}{2\kappa} \Bigg] \\
&\leq \E\left[\exp\left(2\kappa \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\tilde{L}_t(e)\right)\right] \frac{\delta}{|\cE|}\\
&\leq \frac{\delta}{|\cE|}.
\end{align*}

Similarly, we can show that 
\begin{align*}
&\mathbb{P}\Bigg[\sum_{t = 1}^T 2 \kappa \frac{p_t(e)}{P_t(e) + \kappa}1_{r_t(x_t)>0} 1_{r(x_t)<0} c\\ 
&\quad \left(\tilde{L}_t(e) - L_t(e) \right) > \frac{\log\left(\frac{|\cE|}{\delta}\right)}{2\kappa} \Bigg] \leq \frac{\delta}{|\cE|}.
\end{align*}

By taking a union bound over all $e \in \cE$, it now follows that with probability at least $1 - 2\delta$, 
\begin{align*}
% &\sum_{t = 1}^T \sum_{e \in \cE} p_t(e) \tilde{L}_t(e)^2 \\
% &= \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) \left[1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} + 1_{r(x_t)<0} c \right]}{P_t(e) + \kappa}\tilde{L}_t(e)\\
&\sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}\tilde{L}_t(e) \\
&\quad + \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa}\tilde{L}_t(e) \\
& = \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa}L_t(e) + \\
&\quad + \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa}L_t(e)  + \frac{|\cE|\log\left(\frac{|\cE|}{\delta}\right)}{\kappa} \\
&\leq \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c}^2 }{P_t(e) + \kappa} + \\
&\quad + \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c^2 }{P_t(e) + \kappa} + \frac{|\cE|\log\left(\frac{|\cE|}{\delta}\right)}{\kappa}. \\
\end{align*}

Putting all the pieces together, we can now say that
\begin{align*}
&\sum_{t = 1}^T L_t(e_t) \leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} \\
&\quad + \sum_{t = 1}^T \sum_{e \in \cE} p_t(e) L_t(e) \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right]  \\ 
&\quad + \sum_{t = 1}^T \sum_{e \in \cE}\frac{\kappa p_t(e) L_t(e)}{P_t(e) + \kappa} + \sum_{t = 1}^T \tilde{L}_t(e^*) + \frac{\log|\cE|}{\eta}\\ 
&\quad + \frac{\eta}{2} \Bigg[\sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c}^2 }{P_t(e) + \kappa} + \\
&\quad + \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c^2 }{P_t(e) + \kappa} + \frac{|\cE|\log\left(\frac{|\cE|}{\delta}\right)}{\kappa} \Bigg].
\end{align*}

Now consider the second term in the upper bound: $\sum_{t = 1}^T \sum_{e \in \cE} p_t(e) L_t(e) \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right]$. We can decompose this as follows:
\begin{align*}
& \sum_{e \in \cE} p_t(e) L_t(e) \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right] \\
&= \sum_{e \in \cE} p_t(e)  \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right] \\
&\quad + \sum_{e \in \cE} p_t(e)  c 1_{r(x_t) < 0}  \left[ \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa}  \right]\\
&= X_t(e) + Y_t(e).
\end{align*}
Notice that each of $X_t(e)$ and $Y_t(e)$ is a martingale-difference with $|X_t| \leq |\cE| \bar{c}$ and $|Y_t| \leq |\cE| c$. 

Moreover, we can bound the conditional variance as:
\begin{align*}
&\E\Bigg[X_t(e)^2 | \mathcal{F}_t \Bigg] \\
&= \E\Bigg[\Bigg( \sum_{e \in \cE} p_t(e)  \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{P_t(e) - 1_{e \in O_t}}{P_t(e) + \kappa} \Bigg)^2 | \mathcal{F}_t \Bigg] \\
&\leq \E\Bigg[\Bigg( \sum_{e \in \cE} p_t(e)  \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0} \left[ \frac{1_{e \in O_t}}{P_t(e) + \kappa}  \right] \Bigg)^2 | \mathcal{F}_t \Bigg] \\
&= \E\Bigg[\sum_{e, e' \in \cE}   \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e)p_t(e')1_{e \in O_t}1_{e' \in O_t}}{(P_t(e) + \kappa)(P_t(e') + \kappa)}    | \mathcal{F}_t \Bigg] \\
&\leq \E\Bigg[\sum_{e, e' \in \cE}   \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e)p_t(e')1_{e \in O_t}}{(P_t(e) + \kappa)(P_t(e') + \kappa)}    | \mathcal{F}_t \Bigg] \\
&= \sum_{e, e' \in \cE}   \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e)p_t(e')P_t(e)}{(P_t(e) + \kappa)(P_t(e') + \kappa)}   \\
&\leq \sum_{e, e' \in \cE}  p_t(e) \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e')}{P_t(e') + \kappa} 
\end{align*}

By Freedman's inequality, it follows that with probability at least $1-\delta$, for any $\gamma \leq \frac{1}{|\cE| \bar{c}}$,
\begin{align*}
&\sum_{t = 1}^T X_t(e) \leq \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} \\
&\quad + (e - 2) \gamma \sum_{t = 1}^T \sum_{e' \in \cE} \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e')}{P_t(e') + \kappa}.
\end{align*}
We can bound term $Y_t$ similarly to say that with
probability at least $1-\delta$, for any $\gamma' \leq \frac{1}{|\cE|c}$,
\begin{align*}
&\sum_{t = 1}^T Y_t(e) \leq \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'} \\
&\quad + (e - 2) \gamma' \sum_{t = 1}^T \sum_{e' \in \cE} c 1_{r(x_t) <0}  \frac{p_t(e')}{P_t(e') + \kappa}.
\end{align*}

It now follows that with probability at least $1 - 4 \delta$, for any $\gamma \leq \frac{1}{|\cE| \bar{c}}$ and  $\gamma' \leq \frac{1}{|\cE|c}$,
\begin{align*}
&\sum_{t = 1}^T L_t(e_t) \leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} \\
&\quad + \sum_{t = 1}^T \sum_{e \in \cE}\frac{\kappa p_t(e) L_t(e)}{P_t(e) + \kappa} + \sum_{t = 1}^T \tilde{L}_t(e^*) + \frac{\log|\cE|}{\eta}\\ 
&\quad + \frac{\eta}{2} \Bigg[\sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c}^2 }{P_t(e) + \kappa} \\
&\quad + \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c^2 }{P_t(e) + \kappa} + \frac{|\cE|\log\left(\frac{|\cE|}{\delta}\right)}{\kappa} \Bigg] \\
&\quad + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'} \\
&\quad + (e - 2) \gamma \sum_{t = 1}^T \sum_{e' \in \cE} \bar{c}1_{r_t(x_t)>0} 1_{r(x_t) >0}  \frac{p_t(e')}{P_t(e') + \kappa} \\
&\quad + (e - 2) \gamma' \sum_{t = 1}^T \sum_{e' \in \cE} c 1_{r(x_t) <0}  \frac{p_t(e')}{P_t(e') + \kappa} \\
&= \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} + \sum_{t = 1}^T \tilde{L}_t(e^*) + \frac{\log|\cE|}{\eta}\\ 
&\quad + \left(\frac{\eta}{2} \bar{c} + \kappa + (e-2) \gamma  \right) \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa} \\
&\quad + \left(\frac{\eta}{2} c + \kappa + (e-2) \gamma' \right) \sum_{t = 1}^T \sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa} \\
&\quad + \frac{\eta}{2}\frac{|\cE|\log\left(\frac{|\cE|}{\delta}\right)}{\kappa} + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'}.
\end{align*}

By Lemma 1 in \citep{KocakNeuValkoMunos2014},
\begin{align*}
&\sum_{e\in \cE} \frac{p_t(e) 1_{r_t(x_t)>0} 1_{r(x_t)>0} \bar{c} }{P_t(e) + \kappa} \leq \sum_{e\in \cE_{\text{acc},t}} \frac{p_t(e) \bar{c} }{P_t(e) + \kappa} \\
&\leq 2 \bar{c} \left[\log\left( 1 + \left\lceil\frac{|\cE_{\text{acc},t}|^2}{\kappa}\right\rceil + |\cE_{\text{acc},t}|\right) + 1 \right],
\end{align*}
and 
\begin{align*}
&\sum_{e\in \cE} \frac{p_t(e) 1_{r(x_t)<0} c }{P_t(e) + \kappa} \leq \sum_{e\in \cE_{\text{abs},t}} \frac{p_t(e) c }{P_t(e) + \kappa} \\
&\leq 2 c \left[\log\left( 1 + \left\lceil\frac{|\cE_{\text{abs},t}|^2}{\kappa}\right\rceil + |\cE_{\text{abs},t}|\right) + 1\right].
\end{align*}

Thus, we have shown that
\begin{align*}
&\sum_{t = 1}^T L_t(e_t) \leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}}\\
& \quad + \sum_{t = 1}^T \tilde{L}_t(e^*) + \frac{\log|\cE|}{\eta}\\ 
&\quad + \left(\frac{\eta}{2} \bar{c} + \kappa + (e-2) \gamma  \right) \\
&\quad \quad \sum_{t = 1}^T 2 \bar{c} \left[\log\left( 1 + \left\lceil\frac{|\cE_{\text{acc},t}|^2}{\kappa}\right\rceil + |\cE_{\text{acc},t}|\right) + 1 \right] \\
&\quad + \left(\frac{\eta}{2} c + \kappa + (e-2) \gamma' \right) \\
&\quad \quad \sum_{t = 1}^T 2 c \left[\log\left( 1 + \left\lceil\frac{|\cE_{\text{abs},t}|^2}{\kappa}\right\rceil + |\cE_{\text{abs},t}|\right) + 1\right] \\
&\quad + \frac{\eta}{2}\frac{|\cE|\log\left(\frac{|\cE|}{\delta}\right)}{\kappa} + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'} \\
&\leq \max\{c,\bar{c}\} \sqrt{\frac{T \log(1/\delta)}{2}} + \sum_{t = 1}^T L_t(e^*) + \frac{\log|\cE|}{\eta}\\ 
&\quad + \left(\frac{\eta}{2} \bar{c} + \kappa + (e-2) \gamma  \right) \\
&\quad \quad \sum_{t = 1}^T 2 \bar{c} \left[\log\left( 1 + \left\lceil\frac{|\cE_{\text{acc},t}|^2}{\kappa}\right\rceil + |\cE_{\text{acc},t}|\right) + 1 \right] \\
&\quad + \left(\frac{\eta}{2} c + \kappa + (e-2) \gamma' \right) \\
&\quad \quad \sum_{t = 1}^T 2 c \left[\log\left( 1 + \left\lceil\frac{|\cE_{\text{abs},t}|^2}{\kappa}\right\rceil + |\cE_{\text{abs},t}|\right) + 1\right] \\
&\quad + \frac{3\eta }{4}\frac{|\cE|\log\left(\frac{|\cE|}{\delta}\right)}{\kappa} + \frac{\log\left(\frac{1}{\delta}\right)}{\gamma} +  \frac{\log\left(\frac{1}{\delta}\right)}{\gamma'}.
\end{align*}


\end{proof}


\section{Proofs of theorems in the stochastic setting}
\label{app:stochastic}
In this section,  we prove the theorems  in the stochastic setting of Section~\ref{sec:stochastic} in the following order: UCB-N, UCB-D, and UCB-H. 

\begin{reptheorem}{th:ucbn}
The regret of the UCB-N after $T$ steps is upper bounded by
\begin{align*}
&\E \Bigg[ \sum_{t = 1}^T L( h_{I_t}, r_{I_t}) - \sum_{t = 1}^T  L( h_*, r_*)\Bigg]\\
&\leq \sum_{e_j \in \cE} \left(  \frac{ 2\beta \log(T)P_j(\mathcal{T})}{(\mu_{h_j, r_j}-\mu_{h_*,r_*})Q_j(\mathcal{T})}  + \frac{\beta}{\beta -2} \right)
\end{align*}
where $\mathcal{T}=\argmax_t \frac{P_j(t)}{Q_j(t)}$.
\end{reptheorem}
\begin{proof}
The regret can be written as follows
\begin{align*}
&\E \Bigg[ \sum_{t = 1}^T L( h_{I_t}, r_{I_t}) - \sum_{t = 1}^T  L( h_*, r_*)\Bigg] \\
&=  \E \Bigg[ \sum_{t = 1}^T \sum_{j=1}^K  \left (L( h_j, r_j) -   L( h_*, r_*)\right ) 1_{I_t=j}\Bigg] \\
&=  \sum_{t = 1}^T \sum_{j=1}^K  \left ( \mu_{h_j, r_j} -  \mu_{h_*, r_*}\right ) \E  [ 1_{I_t=j} ]
\end{align*}
We then split the sum of the expectation according to $P_j(t-1)$, the number of times expert $j$ was chosen,
\begin{align*}
& \sum_{t = 1}^T \E  [ 1_{I_t=j} ]\\
& =\sum_{t = 1}^T \E  [ 1_{I_t=j} 1_{P_j(t-1)\leq s} ] + \sum_{t = 1}^T \E  [ 1_{I_t=j}1_{P_j(t-1)> s}  ]  \\
 & \leq s + \sum_{t=s+1}^T \E  [ 1_{I_t=j}1_{P_j(t-1)> s}  ]  \\
\end{align*}
Now, if expert $j$ is chosen, that is $I_t=j$, then it must have had the lowest confidence bound  $\widehat{\mu}_{j,t-1}- \mathcal{S}_{j,t-1} \leq \widehat{\mu}_{i,t-1}- \mathcal{S}_{i,t-1}  $ for all $i\in \cE$.  Thus, 
\begin{align} \label{eq:larges}
&\E  [ 1_{I_t=j}1_{P_j(t-1)> s}  ] \nonumber  \\
& \leq  \Pr [\forall i\in \cE,\widehat{\mu}_{j,t-1}\hspace{-1mm}- \hspace{-1mm}\mathcal{S}_{j,t-1} \hspace{-1mm}\leq \hspace{-1mm}\widehat{\mu}_{i,t-1}\hspace{-1mm}-\hspace{-1mm} \mathcal{S}_{i,t-1},P_j(t\hspace{-1mm}-\hspace{-1mm}1)\hspace{-1mm}>\hspace{-1mm}s ] \nonumber \\
&\leq  \Pr [\widehat{\mu}_{j,t-1}- \mathcal{S}_{j,t-1} \leq \widehat{\mu}_{*,t-1}- \mathcal{S}_{*,t-1} ,P_j(t-1)> s ]
\end{align}

where the last inequality comes from the fact that the probability of an intersection of events is less than the probability of one event. 
Now, we can re-write the inequality as
\begin{align*}
0 &\leq \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1}- \widehat{\mu}_{j,t-1}+\mathcal{S}_{j,t-1}\\ &=-\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} + 2\mathcal{S}_{j,t-1} - (\mu_{h_j, r_j}  \\ & - \mu_{h_*,r_*}) + \mu_{h_j, r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1}, 
\end{align*}
that is it can be written as the sum of the following three term
\begin{align*}
  [1]\hspace{4mm} & -\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} \\
  [2] \hspace{4mm}&  2\mathcal{S}_{j,t-1} - (\mu_{h_j, r_j}- \mu_{h_*,r_*}) \\
  [3] \hspace{4mm} & \mu_{h_j, r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1}  \\
\end{align*}
Note that at least one of the three terms is non-negative.  If 
$$s= \ceil*{ \frac{2\beta \log T}{ (\mu_{h_j, r_j}- \mu_{h_*,r_*})^2 }\max_t \frac{P_j(t-1)}{Q_j(t-1)} },$$ then
$P_j(t-1) \geq s \geq  \frac{2\beta \log t}{ (\mu_{h_j, r_j}- \mu_{h_*,r_*}) ^2 }\frac{P_j(t-1)}{Q_j(t-1)}$. This leads to the following set of inequalities:

\begin{align*}
&Q_j(t-1) \geq  \frac{2\beta \log t}{ (\mu_{h_j, r_j}- \mu_{h_*,r_*})^2 } \\ &\implies (\mu_{h_j, r_j}- \mu_{h_*,r_*})^2 \geq  \frac{2\beta \log t}{ Q_j(t-1) } \\
&\implies (\mu_{h_j, r_j}- \mu_{h_*,r_*}) \geq  \sqrt{\frac{2\beta \log t}{ Q_j(t-1) }}  
\end{align*}

The last inequality can be written as $-2\mathcal{S}_{j,t} +(\mu_{h_j, r_j}- \mu_{h_*,r_*})\geq 0$. This implies that at least one of the [1] and [3] terms are non-negative. Thus, we can rewrite the left-hand side of Inequality~\ref{eq:larges} as
\begin{align*}
& \Pr \big[\widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1}\leq\widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t},P_j(t-1)>s   \big] \\
&\leq \Pr \big[ \mu_{h_j, r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1} \geq 0 \big ] \\ &  + \Pr \big[   -\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} \geq 0   \big]\\
\end{align*}

Now by Hoeffding's inequality and the union bound,
\begin{align*}
&\Pr \left( -\mu_{h_*,r_*}+ \widehat{\mu}_{*,t-1}-\mathcal{S}_{*,t-1} \geq 0\right)\\ 
& \leq \Pr \left ( \exists s\in [1,t]: -\mu_{h_*,r_*}+ \widehat{\mu}_{*,s}-\sqrt{\frac{2\beta \log t}{s}} \geq 0    \right )\\
&\leq \sum_{s=1}^t \frac{1}{t^\beta} = \frac{1}{t^{\beta -1}}
\end{align*}

A similar argument holds for the [3] term, that is $\Pr \big( \mu_{h_j, r_j}- \widehat{\mu}_{j,t-1}-\mathcal{S}_{j,t-1} \geq 0\big) \leq \frac{1}{t^{\beta -1}} $ and thus
 \begin{align*}
& \sum_{t = 1}^T   \E \big [ 1_{I_t=j} \big ]  \\ 
&\leq     \frac{2\beta \log T}{ (\mu_{h_j, r_j}- \mu_{h_*,r_*})^2 } \max_t \frac{P_j(t-1)}{Q_j(t-1)}  + 2  \sum_{t=s+1}^T \frac{1}{t^{\beta -1}}  \\ 
& \leq    \frac{2\beta \log T}{ (\mu_{h_j, r_j}- \mu_{h_*,r_*})^2 } \max_t  \frac{P_j(t-1)}{Q_j(t-1)}   + \frac{\beta}{\beta-2}\\
&  \leq \frac{2\beta \log T}{ (\mu_{h_j, r_j}- \mu_{h_*,r_*})^2 }   \frac{P_j(\mathcal{T})}{Q_j(\mathcal{T})}   + \frac{\beta}{\beta-2}
 \end{align*}
 
Using this upper bound of the expectation in the equation $ \sum_{t = 1}^T \sum_{j=1}^K  \left ( \mu_{h_j, r_j} -  \mu_{h_*, r_*}\right ) \E  [ 1_{I_t=j} ]$, we can then conclude the statement of the theorem.

\end{proof}

TODO: add proof of theorem 4

\begin{reptheorem}{th:ucbh}
For any $T>0$, the sleeping regret of UCB-H is upper bounded by
\begin{align*}
& \sum_{t = 1}^T \E [ L( h_{I_t}, r_{I_t} )]  - \min_{\sigma} \sum_{t = 1}^T \E [ L( h_{\sigma(W_t)}, r_{\sigma(W_t)} )]\\
& \leq O \left(  \beta \log T  \sum_{j=2}^{K}\frac{1}{\nu_{j}-\nu_{j-1}} \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})}  \right )  
\end{align*}
where $\mathcal{T}=\argmax_t \frac{P_j(t)}{O_j(t)}$
\end{reptheorem}
\begin{proof}
We initially analyze the regret by focusing on each expert $j$. For $[i]=\{1,2,\ldots,i\}$, let $M_{i,j}$ for $i<j$ denote the number of time expert $j$ was chosen, where some expert in $[i]$ could have been chosen. Then, 
\begin{align*}
\E[M_{i,j}]= \sum_{t = 1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }]
\end{align*}
where $W_t$ is the set of awake experts. Suppose for now that expert $e_j$ is not the all-abstain function $e_\diamond$. We can split the sum over the rounds according to $P_j(t-1)$ the number of times that $j$ was chosen:
\begin{align*}
&\sum_{t = 1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }]\\ &= \sum_{t = 1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{P_j(t-1) \leq s }] \\ &+ \sum_{t = 1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{P_j(t-1) > s }] \\
&  \leq s + \sum_{t=s+1}^T \E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{P_j(t-1) > s }] 
\end{align*}

Now, if expert $j$ was chosen, that is $I_t=j$, then it must have had the lowest confidence bound $\widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1}  $ for all $e_k\in W_t-e_\diamond$ and $ \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq c$.  Let $e_{k^*}=\argmin_{e_k \in W_t \wedge [i] }C(k) $ where $C(k)= \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1} $  if  $e_k\in W_t \wedge [i] - \{e_\diamond \}$ and $C(k)=c$ if $e_\diamond \in  W_t \wedge [i]$. 
\begin{align}\label{eq:largeawakes}
& E[1_{I_t=j}1_{W_t \wedge [i]\neq \emptyset }1_{P_j(t-1) > s }] \nonumber
\\  & \leq \Pr[ \forall e_k\in W_t-e_\diamond, \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1},\nonumber \\
&\hspace{10mm} \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1}\leq c ,W_t \wedge [i]\neq \emptyset,P_j(t-1) > s ]\nonumber \\
& \leq \Pr[ \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq C(k^*) ,\nonumber\\
&\hspace{10mm} W_t \wedge [i]\neq \emptyset,P_j(t-1) > s ]  
\end{align}
where the last inequality comes from the fact that the probability of an intersection of events is less than the probability of one event. If $C(k^*)= \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}$, then we can write the inequality 
\begin{align*}
0& \leq \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}-  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1} \\
& =\widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}-  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1}- \nu_{k^*}+ \nu_{k^*} \\
& - \nu_{j}+ \nu_{j} - \nu_{i}+ \nu_{i}   + \mathscr{S}_{j,t-1} - \mathscr{S}_{j,t-1}\\
& =- \nu_{k^*}+ \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1} + \nu_{j}-  \widehat{\nu}_{j,t-1} - \mathscr{S}_{j,t-1}\\
&+ (\nu_{k^*} - \nu_{i}) - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1} \\
\end{align*}
as the sum of the following three terms
\begin{align*}
  [1]\hspace{4mm} & -\nu_{k^*}+ \widehat{\nu}_{k^*,t-1}-\mathscr{S}_{k^*,t-1} \\
  [2] \hspace{4mm}&  - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1}  +(\nu_{k^*} - \nu_{i})\\
  [3] \hspace{4mm} & \nu_{j}- \widehat{\nu}_{j,t-1}-\mathscr{S}_{j,t-1}  \\
\end{align*}
Note that at least one of the three terms in non-negative.  
If instead $C(k^*)=c$, then we can write the inequality 
\begin{align*}
0& \leq c-  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1} \\
& =c -  \widehat{\nu}_{j,t-1} + \mathscr{S}_{j,t-1} - \nu_{j}+ \nu_{j} \\
&  - \nu_{i}+ \nu_{i}   + \mathscr{S}_{j,t-1} - \mathscr{S}_{j,t-1}\\
& =  \nu_{j}-  \widehat{\nu}_{j,t-1} - \mathscr{S}_{j,t-1}\\
& + (c - \nu_{i}) - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1} \\
\end{align*}
as the sum of the following two terms 
\begin{align*}
  [1'] \hspace{4mm} & \nu_{j}- \widehat{\nu}_{j,t-1}-\mathscr{S}_{j,t-1}  \\
  [2'] \hspace{4mm}&  - (\nu_{j} - \nu_{i}) + 2\mathscr{S}_{j,t-1}  +(c - \nu_{i})\\
\end{align*}

By  choosing $$s=\ceil*{\frac{2\beta \log T}{(\nu_j-\nu_i)^2} \max_t \frac{P_j(t-1)}{O_j(t-1)} },$$  then $P_j(t-1) \geq s \geq \frac{2\beta \log t}{(\nu_j-\nu_i)^2} \frac{P_j(t-1)}{O_j(t-1)}  $. This leads to the following set of inequalities:

\begin{align*}
&O_j(t-1) \geq  \frac{2\beta \log t}{ (\nu_{j}- \nu_{i})^2 } \\ &\implies (\nu_{j}- \nu_{i})^2 \geq  \frac{2\beta \log t}{ O_j(t-1) } \\
&\implies (\nu_{j}- \nu_{i}) \geq  \sqrt{\frac{2\beta \log t}{ O_j(t-1) }} \\
&\implies (\nu_{j}- \nu_{i}) - 2\mathscr{S}_{j,t-1}  \geq 0 
\end{align*}
Notice that by definition of the set $[i]$, $\nu_{k^*} \leq \nu_i$ as well as $c\leq \nu_i$, which implies $-(\nu_{k^*}-\nu_i)\geq 0$ and $-(c-\nu_i)\geq 0$. Thus $$(\nu_{j}- \nu_{i}) - 2\mathscr{S}_{j,t-1}-(\nu_{k^*}-\nu_i)  \geq 0 $$ and  $$(\nu_{j}- \nu_{i}) - 2\mathscr{S}_{j,t-1}-(c-\nu_i)  \geq 0 $$ which is equivalent to term [2] and [2'] being non-positive. Thus, at least one of the  [1] and [3] terms are non-negative and also [1'] must be non-negative.  This implies that we can re-write the right-hand side of Inequality~\ref{eq:largeawakes}  as
\begin{align*}
 &\Pr[ \widehat{\nu}_{j,t-1} -\mathscr{S}_{j,t-1} \leq C(k^*), \\
&\hspace{10mm} W_t \wedge [i]\neq \emptyset,P_j(t-1) > s ]  \\
& \leq  \Pr[-\nu_{k^*}+ \widehat{\nu}_{k^*,t-1}-\mathscr{S}_{k^*,t-1}  \geq 0 ] \\ 
& \hspace{2mm} +\Pr[\nu_{j}- \widehat{\nu}_{j,t-1}-\mathscr{S}_{j,t-1}\geq 0]
\end{align*}
Now by Hoeffding's inequality and the union bound, 
\begin{align*}
&\Pr \big( -\nu_{k^*}+ \widehat{\nu}_{k^*,t-1}-\mathscr{S}_{k^*,t-1} \geq 0\big)\\ 
& \leq \Pr  \left ( \exists s\in [1,t]: -\nu_{k^*}+ \widehat{\nu}_{k^*,s}-\sqrt{\frac{2\beta \log t}{s}} \geq 0    \right )\\
&\leq \sum_{s=1}^t \frac{1}{t^\beta} = \frac{1}{t^{\beta -1}}
\end{align*}
A similar argument holds for  [3] and [1'] term and thus,
\begin{align*}
&\E[M_{i,j}] \\
& \leq  \frac{2\beta \log T}{(\nu_j-\nu_i)^2} \max_t \frac{P_j(t-1)}{O_j(t-1)} + 2 \sum_{t=s+1}^T \frac{1}{t^{\beta-1}}\\ 
& \leq  \frac{2\beta \log T}{(\nu_j-\nu_i)^2}  \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})} + \frac{\beta}{\beta-2}\\ 
\end{align*}

We now assume that expert $j=j_\diamond$ is the all-abstain expert. Thus if expert $j_\diamond$ is chosen, then $c \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1}  $ for all $e_k\in W_t-e_\diamond$. Let $e_{k^*}=\argmin_{e_k \in W_t \wedge [i]}  \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1} $ so that
\begin{align*}
 & \E[1_{I_t=j_\diamond}1_{W_t \wedge [i]\neq \emptyset}]\\ 
 & \leq \Pr[\forall e_k \in W_t-e_\diamond, c \leq \widehat{\nu}_{k,t-1} -\mathscr{S}_{k,t-1},W_t \wedge [i]\neq \emptyset  ]\\
 & \leq  \Pr[  c \leq \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}, W_t \wedge [i]\neq \emptyset  ]
\end{align*}
We can re-write the inequality as
\begin{align*}
& 0 \leq \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1} -c \\
& = - \nu_{k^*} + \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1} -c +\nu_{k^*} \\
\end{align*}
Since by definition of the set $[i]$ and the fact that $j_\diamond>i$, $c -\nu_{k^*}\geq 0$, then  $- \nu_{k^*} + \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}\geq 0$. Similarly as before, by Hoeffding's and the union bound, $\Pr [- \mu_{k^*} + \widehat{\nu}_{k^*,t-1} -\mathscr{S}_{k^*,t-1}\geq 0] \leq \frac{1}{t^{\beta-1}}$. Thus, when expert $j$ is the all-abstain function, for $i<j$,
\begin{align*}
\E[M_{i,j_\diamond} ] & = \sum_{t = 1}^T \E[1_{I_t=j_\diamond}1_{W_t \wedge [i]\neq \emptyset}]\\ & \leq \sum_{t = 1}^T \frac{1}{t^{\beta-1}} \leq \frac{\beta}{2(\beta-2)}
\end{align*}

Now that we have bounded the expectation of $M_{i,j}$, we turn to the sleeping regret, which can be written as
\begin{align*}
&\E \left [\sum_{j=2}^{K} \sum_{i=1}^{j-1} ( M_{i,j}-  M_{i-1,j} ) (\nu_j-\nu_i) \right ] \\
&=\E \left [ \sum_{j=2}^{K} \sum_{i=1}^{j-1}  M_{i,j}  (\nu_{i+1}-\nu_i) \right ]\\
\end{align*}
by simply regrouping terms and using Lemma~\ref{lemma:sleepingregretreform}. 
Then by using the bound on the expectation of $M_{i,j}$, 
\begin{align*}
& \sum_{j=2}^{K} \sum_{i=1}^{j-1}  \E[M_{i,j}] (\nu_{i+1}-\nu_i) \\
&\leq \sum_{j=2}^{K} 2\beta \log T \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})}  \sum_{i=1}^{j-1} \frac{1}{(\nu_j-\nu_i)^2} (\nu_{i+1}-\nu_i) \\ &- 2\beta \log T \frac{P_{j_\diamond}(\mathcal{T})}{O_{j_\diamond}(\mathcal{T})}  \sum_{i=1}^{{j_\diamond}-1} \frac{1}{(\nu_{j_\diamond}-\nu_i)^2} (\nu_{i+1}-\nu_i)  +O(1)
\end{align*}
where we added and subtracted the factor $$2\beta \log T \frac{P_{j_\diamond}(\mathcal{T})}{O_{j_\diamond}(\mathcal{T})}  \sum_{i=1}^{{j_\diamond}-1} \frac{1}{(\nu_{j_\diamond}-\nu_i)^2} (\nu_{i+1}-\nu_i)$$ of the all-abstain expert and used the fact that $E[M_{i,j_\diamond}]=O(1)$.  Note that $O_{j_\diamond}$ is the number of times the all-abstain expert was observed and $P_{j_\diamond}$ is the number of times it was chosen.
We now focus on the first term $ \sum_{j=1}^{K-1} 2\beta \log T \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})}  \sum_{i=1}^{j-1} \frac{1}{(\nu_j-\nu_i)^2} (\nu_{i+1}-\nu_i)$, which  we can write it as
\begin{align*}
 2\beta \log T \sum_{j=1}^{K-1} \sum_{i=1}^{j-1} \frac{(\nu_{i+1}-\nu_i ) }{(\nu_j-\nu_i)^2}  \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})} 
\end{align*}
We then apply Lemma ~\ref{lemma3}  to conclude the statement of the theorem. 

TODO: find better way to keep track to $j_\diamond$
\end{proof}

\begin{lemma}\label{lemma3}
For $\nu_1 \leq \nu_2 \leq \ldots \leq \nu _K$, then
\begin{align*}
& \sum_{j=2}^{K} \sum_{i=1}^{j-1}  \frac{(\nu_{i+1}-\nu_i ) }{(\nu_j-\nu_i)^2}  \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})}  \\
& \leq 2 \sum_{j=2}^{K}\frac{1}{\nu_{j}-\nu_{j-1}} \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})} \\
\end{align*}
\end{lemma}
\begin{proof}
Consider the following simple change of variable
\begin{align*}
&\sum_{j=2}^{K } \sum_{i=1}^{j-1} \frac{(\nu_{i+1}-\nu_i )}{(\nu_j-\nu_i)^2}  \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})} \\ 
&=\sum_{j=2}^{K } \sum_{i=1}^{j-1} \frac{(\nu -\nu + \nu_{i+1}-\nu_i )}{(\nu -\nu + \nu_j-\nu_i)^2} \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})} \\
&= \sum_{j=2}^{K } \sum_{i=1}^{j-1} \frac{(\nu -\nu_i -(\nu - \nu_{i+1}) ) }{(\nu-\nu_i - (\nu - \nu_j))^2}\frac{P_j(\mathcal{T})}{O_j(\mathcal{T})} \\
\end{align*}
where $\nu > \max_i \nu_i $. Letting $\tilde{\nu}_i= \nu -\nu_i$,  then $\tilde{\nu}_1 \geq \tilde{\nu}_2 \geq \ldots \geq  \tilde{\nu}_K $ and 
\begin{align*}
& \sum_{i=1}^{j-1} \frac{(\nu -\nu_i -(\nu - \nu_{i+1}) ) }{(\nu-\nu_i - (\nu - \nu_j))^2}  \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})}\\
& =\sum_{i=1}^{j-1} \frac{(\tilde{\nu}_i -\tilde{\nu}_{i+1} ) }{(\tilde{\nu}_i - \tilde{\nu}_j)^2} \frac{P_j(\mathcal{T})}{O_j(\mathcal{T})} \\
\end{align*}
so that we can directly apply Lemma~\ref{lemma:newlemma3}
\end{proof}

\begin{lemma} \label{lemma:newlemma3}
For $\mu_1 \geq \mu_2 \geq \ldots \geq \mu _K$, let $F_j>0$ for all $j\in[1,K]$ and define $\Delta_{i,j}=\mu_i-\mu_j$, then
\begin{align*}
& \sum_{j=2}^{K} \sum_{i=1}^{j-1} \frac{F_j}{\Delta_{i,j}^2}  \Delta_{i,i+1}\leq 2\sum_{j=2}^K F_j \Delta_{j-1,j}^{-1}  \\
\end{align*}
\end{lemma}
\begin{proof}
The following proof is adapted from Lemma 3 of \citet{KleinbergNiculescuSharma2008} to include the factor $F_j$. By changing the order of summation,
\begin{align*}
 &\sum_{j=2}^{K} \sum_{i=1}^{j-1} \frac{F_j}{\Delta_{i,j}^2}  \Delta_{i,i+1} = \sum_{i=1}^{K-1} \Delta_{i,i+1} \sum_{j:j>i}\frac{F_j}{\Delta_{i,j}^2}
\end{align*}
We then analyze the second sum by fixing an expert $i\in[n]$,  
\begin{align*}
&\sum_{j:j>i}\frac{F_j}{\Delta_{i,j}^2}\\ &= \sum_{j=2}^K 1_{j>i}\frac{F_j}{\Delta_{i,j}^2} \\
& =\int_{x=0}^\infty \#\{j:j>i,\Delta_{i,j}^{-2}F_j \geq x \}dx \\
& = \int_{x=0}^\infty \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq x^{-1/2} \}dx \\
& = -2 \int_{y=\infty}^0 \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq y \}y^{-3}dy \\
& = 2 \int_{y=0}^\infty \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq y \}y^{-3}dy \\
\end{align*}
where we changed variables of integration $x^{-1/2}=y$.

Thus combining the above, 
\begin{align*}
 & \sum_{i=1}^{K-1} \Delta_{i,i+1} \sum_{j:j>i}\frac{F_j}{\Delta_{i,j}^2} \\
 & = 2\sum_{i=1}^{K-1}  \Delta_{i,i+1}  \int_{y=0}^\infty \hspace*{-1mm} \#\{j:j>i,\Delta_{i,j}F_j^{-1/2} \leq y \}y^{-3}dy  \\
 & =   2  \hspace*{-1mm}  \int_{y=0}^\infty \hspace*{-2mm}  y^{-3}  \left( \sum_{i=1}^{K-1}  \Delta_{i,i+1}  \#\{j\hspace*{-1mm}:\hspace*{-1mm}j\hspace*{-1mm}>\hspace*{-1mm}i,\Delta_{i,j}F_j^{-1/2} \leq y \}  \hspace*{-1mm}  \right)  dy \\
\end{align*}
where we changed the order of integration and summation. We then expand $\#\{\}$ into a sum of indicator functions
\begin{align*}
 &  2  \hspace*{-1mm}  \int_{y=0}^\infty \hspace*{-2mm}  y^{-3}  \left( \sum_{i=1}^{K-1}  \Delta_{i,i+1}  \#\{j\hspace*{-1mm}:\hspace*{-1mm}j\hspace*{-1mm}>\hspace*{-1mm}i,\Delta_{i,j}F_j^{-1/2} \leq y \}  \hspace*{-1mm}  \right)  dy \\
& = 2  \int_{y=0}^\infty y^{-3}  \left( \sum_{i=1}^{K-1}  \Delta_{i,i+1}  \sum_{j=i+1}^K 1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y}  \right)  dy \\
& = 2  \int_{y=0}^\infty y^{-3}  \left(\sum_{j=2}^K   \sum_{i=1}^{j-1}  \Delta_{i,i+1}  1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y}  \right)  dy \\
\end{align*}
where the last equation is by changing the order of summation.
For an expert $j$ and $y\geq 0$, we define 
\begin{align*}
i_{y}(j):=\argmin \{i: i\leq j, \Delta_{i,j}F_j^{-1/2}\leq y \}
\end{align*}
In other words, $i_{y}(j)$ is the the minimum numbered expert $i\leq j$ such that $\Delta_{i,j}F_j^{-1/2}$ is no more than $y$. Thus,  
\begin{align*}
&\sum_{j=2}^K   \sum_{i=1}^{j-1}  \Delta_{i,i+1}  1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y} \\
&=\sum_{j=2}^K   \sum_{i=i_{y}(j)}^{j-1}  \Delta_{i,i+1} \\
& =\sum_{j=2}^K  ( \mu_{i_{y}(j)} - \mu_j)
\end{align*}
By using this fact and changing the order of summation and integration
\begin{align*}
 &2  \int_{y=0}^\infty y^{-3}  \left(\sum_{j=2}^K   \sum_{i=1}^{j-1}  \Delta_{i,i+1}  1_{j>i,\Delta_{i,j}F_j^{-1/2} \leq y}  \right)  dy \\
 & = 2\sum_{j=2}^K \int_{y=0}^\infty y^{-3} ( \mu_{i_{y}(j)} - \mu_j)\\
 & =  2\sum_{j=2}^K \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-3} ( \mu_{i_{y}(j)} - \mu_j) \\
\end{align*}
where the last equality holds because for values y less than $\Delta_{j-1,j}F_j^{-1/2}$, $i_y(j)=j$ and the integrand is equal to zero. Notice also that $( \mu_{i_{y}(j)} - \mu_j) F_j^{-1/2} \leq y $, hence
\begin{align*}
& 2\sum_{j=2}^K \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-3} ( \mu_{i_{y}(j)} - \mu_j)\\
& = 2\sum_{j=2}^K \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-3} y F_j^{1/2} dy\\
& = 2\sum_{j=2}^K  F_j^{1/2}  \int_{y=\Delta_{j-1,j}F_j^{-1/2}}^\infty y^{-2}  dy\\
& =2\sum_{j=2}^K  F_j  \Delta_{j-1,j}^{-1}\\
\end{align*}
which is the bound the lemma. 
\end{proof}


\begin{lemma}\label{lemma:sleepingregretreform}
The sleeping regret can be written in terms of $M_{i,j}$ in the following way
\begin{align*}
& \sum_{t = 1}^T \E[ L ( h_{I_t}, r_{I_t} )] - \min_{\sigma} \sum_{t = 1}^T \E [ L ( h_{\sigma(W_t)}, r_{\sigma(W_t)} )]\\
&=\E \left [\sum_{j=2}^{K} \sum_{i=1}^{j-1} ( M_{i,j}-  M_{i-1,j} ) (\nu_j-\nu_i) \right ] \\
\end{align*}
\end{lemma}
\begin{proof}
Let $I^*_t$ be the index of the best ranked expert that is available at time $t$. 
\begin{align*}
&\sum_{t = 1}^T \E[ L ( h_{I_t}, r_{I_t} )] - \min_{\sigma} \sum_{t = 1}^T \E [ L ( h_{\sigma(W_t)}, r_{\sigma(W_t)} )] \\
&= \E \left[ \sum_{t = 1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1} 1_{I_t=j}1_{I^*_t=i}[L ( h_{j}, r_{j} ) -  L ( h_i, r_i )]  \right] \\
&= \sum_{t = 1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1} \E \left[ 1_{I_t=j}1_{I^*_t=i}[L ( h_{j}, r_{j} ) -  L ( h_i, r_i )]  \right] \\
\end{align*}
We now analyze $\E \left[ 1_{I_t=j}1_{I^*_t=i}[L ( h_{j}, r_{j} ) -  L ( h_i, r_i )]  \right] $ and assume for now that $j$ and $i$ are not the all-reject expert. Then,
\begin{align*}
&\E \left[ 1_{I_t=j}1_{I^*_t=i}[L ( h_{j}, r_{j} ) -  L ( h_i, r_i )]  \right] \\
& = \E \left[ 1_{I_t=j}1_{I^*_t=i}L ( h_{j}, r_{j} )\right ] - \E \left[ 1_{I_t=j}1_{I^*_t=i} L ( h_i, r_i )  \right]\\
& = \E \left[ 1_{I_t=j}1_{I^*_t=i}L ( h_{j}, r_{j} )|r_j>0\right ]\Pr(r_j>0)\\ 
& \quad - \E \left[ 1_{I_t=j}1_{I^*_t=i} L ( h_i, r_i ) |r_i>0 \right]\Pr[r_i>0]\\
\end{align*}
where we used the fact that expert $i$ and expert $j$ are not the all-reject function and hence they are only chosen if they accept, which implies that $\E \left[ 1_{I_t=j}1_{I^*_t=i} L ( h_i, r_i ) |r_i\leq 0 \right]=0 $ and $\E \left[ 1_{I_t=j}1_{I^*_t=i} L ( h_j, r_j ) |r_j\leq 0 \right]=0 $. By independence of expectations, 
\begin{align*}
& \E \left[ 1_{I_t=j}1_{I^*_t=i}L ( h_{j}, r_{j} )|r_j>0\right ]\Pr(r_j>0) \\
&\quad - \E \left[ 1_{I_t=j}1_{I^*_t=i} L ( h_i, r_i ) |r_i>0 \right]\Pr[r_i>0]\\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i} |r_j>0\right ] \E \left[L ( h_{j}, r_{j} )|r_j>0\right ]\Pr(r_j>0) \\
&\quad- \E \left[ 1_{I_t=j}1_{I^*_t=i}|r_i>0 \right] \E \left[ L ( h_i, r_i ) |r_i>0 \right]\Pr[r_i>0] \\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ] \E \left[L ( h_{j}, r_{j} )|r_j>0\right ] \\ &\quad - \E \left[ 1_{I_t=j}1_{I^*_t=i}  \right] \E \left[ L ( h_i, r_i ) |r_i>0 \right]  \\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ]  ( \E \left[L ( h_{j}, r_{j} )|r_j>0\right ] \\ 
&\quad - \E \left[ L ( h_i, r_i ) |r_i>0 \right]  )  \\
&=\E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ] \left ( \nu_j -\nu_i\right )  \\
\end{align*}
Thus,  
\begin{align*}
& \sum_{t = 1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1}  \E \left[ 1_{I_t=j}1_{I^*_t=i}[L ( h_{j}, r_{j} ) -  L ( h_i, r_i )]  \right]\\
 & = \sum_{t = 1}^T  \sum_{j=2}^K \sum_{i=1}^{j-1} \E \left[ 1_{I_t=j}1_{I^*_t=i}  \right ] \left ( \nu_j -\nu_i\right ) 
\end{align*}
It is then easy to conclude the statement in the theorem. 

Suppose instead that expert $j$ is the all-reject, then 
\begin{align*}
&\E \left[ 1_{I_t=j}1_{I^*_t=i}[L ( h_{j}, r_{j} ) -  L ( h_i, r_i )]  \right]\\ 
& =\E \left[ 1_{I_t=j}1_{I^*_t=i}L ( h_{j}, r_{j} )|r_j\leq 0\right] \Pr(r_j\leq 0) \\ &\quad - \E \left[ 1_{I_t=j}1_{I^*_t=i} L ( h_i, r_i )|r_i>0\right ]\Pr(r_i>0)  
\end{align*}
and the rest of the proof proceed in a similar way as above. 
\end{proof}


\section{Follow-the-Leader in the Stochastic Setting}

In this section, we show the regret bound of Follow-the-Leader (FTL) algorithm in the stochastic setting. At each round, FTL chooses the expert with the smallest empirical loss and updates the empirical loss of all the experts. See Algorithm~\ref{alg:ftl} for the pseudocode. 

\begin{algorithm}
  \caption{Follow-the-Leader }         
\label{alg:ftl}      
\begin{algorithmic}  
 \FOR{$t \geq 1$}
 \STATE $(h_{I_t}, r_{I_t}) \leftarrow \argmin_{(h, r)\in \cE} \Big \{ \widehat{L}_{h, r,t} \Big \}$
 \FOR {$(h',r')\in \cE$}
  \STATE $\widehat{L}_{h',r',t+1} \leftarrow \frac{ L(h',r')}{t+1} + (1-\frac{1}{t+1}) \widehat{L}_{h',r',t}$ 
 \ENDFOR
 \ENDFOR
\end{algorithmic}
\end{algorithm}
\begin{theorem}
The regret of the FTL after $T$ steps is upper bounded by
\begin{align*}
& \E \left[ \sum_{t = 1}^T  L( h_t, r_t) -  \min_{(h, r)\in\cE}  L(h, r)  \right] \\ & \leq  \frac{4 \max_{(h_j, r_j)\in \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*})  }{\min_{(h_j, r_j)\in \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*}) ^2} 
\end{align*} 
\end{theorem}
\begin{proof}
The regret of the FTL algorithm can be written as
\begin{align*}
&\E \Bigg [ \sum_{t = 1}^T  (L( h_{I_t}, r_{I_t}) - L( h^*,r^*) )1_{I_t\neq *} \Bigg ]\\
\leq & \sum_{t = 1}^T \max_{(h_j, r_j)\in \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*}) \E  [ 1_{I_t\neq *}] \\
\leq &\max_{(h_j, r_j)\in \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*})  \sum_{t = 1}^T  \E  [ 1_{I_t\neq *}] 
\end{align*}
Using Lemma 1 of  \citet{CaronKvetonLelargeBhagat2012}, the following holds
\begin{align*}
 \sum_{t = 1}^T  \E  [ 1_{I_t\neq *}] \leq &    \sum_{t = 1}^T  2e^{-t\min_{(h_j, r_j)\in  \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*})^2 /2 } \\
 = &     \frac{2(1-e^{-T\min_{e_j\in  \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*})^2 /2 })}{e^{\min_{(h_j, r_j)\in  \cE}  (\mu_{h_j, r_j}-\mu_{h_*,r_*})^2 /2 }-1}   \\
 \leq &  \frac{2}{e^{\min_{ (h_j, r_j)\in  \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*})^2 /2 }-1} \\
  \leq &  \frac{2}{\min_{(h_j, r_j)\in  \cE} (\mu_{h_j, r_j}-\mu_{h_*,r_*})^2 /2 } 
\end{align*}
\end{proof}
